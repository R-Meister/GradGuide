{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOybkXW0AoDWmhxyQVtGAkQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R-Meister/GradGuide/blob/main/BigData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Module 1: Data Ingestion & ETL\n",
        "This notebook handles loading raw weather data from CSV into Spark, performing basic cleaning,\n",
        "and writing to Delta Lake Bronze tables with proper partitioning.\n",
        "\n",
        "\n",
        "# Install required packages for Google Colab\n",
        "!pip install pyspark==3.4.0 delta-spark==2.4.0 pyarrow\n",
        "\"\"\"\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
        "from datetime import datetime\n",
        "\n",
        "# Create a Spark session with Delta Lake support\n",
        "def create_spark_session():\n",
        "    \"\"\"Create and return a Spark session with Delta Lake support.\"\"\"\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Weather Data Processing\") \\\n",
        "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
        "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # For Google Colab, display Spark UI link\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        display(output.serve_kernel_port_as_window(spark.sparkContext.uiWebUrl))\n",
        "        print(f\"Spark Web UI available at: {spark.sparkContext.uiWebUrl}\")\n",
        "    except:\n",
        "        print(\"Spark UI link not available\")\n",
        "\n",
        "    return spark\n",
        "\n",
        "# Function to check for the sample data and provide file upload in Colab\n",
        "def check_for_data():\n",
        "    \"\"\"Check if the dataset exists and provide file upload widget in Colab.\"\"\"\n",
        "    dataset_path = '/content/GlobalLandTemperaturesByCity.csv'\n",
        "\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"\\nDATA FILE NOT FOUND: '{dataset_path}'\")\n",
        "        print(\"\\nPlease upload the dataset using the file upload widget below:\")\n",
        "\n",
        "        # For Google Colab, provide file upload widgets\n",
        "        from google.colab import files\n",
        "        print(\"\\nUploading file through Colab interface...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        # Check for correct file upload\n",
        "        if 'GlobalLandTemperaturesByCity.csv' in uploaded:\n",
        "            print(\"Upload complete! Dataset is now available.\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Required file not uploaded. Please upload 'GlobalLandTemperaturesByCity.csv'.\")\n",
        "            return False\n",
        "    else:\n",
        "        print(f\"Dataset found: '{dataset_path}'\")\n",
        "        return True\n",
        "\n",
        "# Define schema for the CSV file\n",
        "def define_schema():\n",
        "    \"\"\"Define and return the schema for the weather data.\"\"\"\n",
        "    schema = StructType([\n",
        "        StructField(\"dt\", StringType(), True),\n",
        "        StructField(\"AverageTemperature\", DoubleType(), True),\n",
        "        StructField(\"AverageTemperatureUncertainty\", DoubleType(), True),\n",
        "        StructField(\"City\", StringType(), True),\n",
        "        StructField(\"Country\", StringType(), True),\n",
        "        StructField(\"Latitude\", StringType(), True),\n",
        "        StructField(\"Longitude\", StringType(), True)\n",
        "    ])\n",
        "    return schema\n",
        "\n",
        "# Load data with the defined schema\n",
        "def load_data(spark, schema):\n",
        "    \"\"\"Load data from CSV file using the provided schema.\"\"\"\n",
        "    df = spark.read.format(\"csv\") \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
        "        .schema(schema) \\\n",
        "        .load(\"GlobalLandTemperaturesByCity.csv\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Basic cleaning and transformation\n",
        "def clean_data(df):\n",
        "    \"\"\"Perform basic cleaning and transformation of the data.\"\"\"\n",
        "    # Convert date string to date type\n",
        "    df = df.withColumn(\"dt\", F.to_date(F.col(\"dt\")))\n",
        "\n",
        "    # Extract year and month for partitioning\n",
        "    df = df.withColumn(\"year\", F.year(F.col(\"dt\")))\n",
        "    df = df.withColumn(\"month\", F.month(F.col(\"dt\")))\n",
        "\n",
        "    # Handle missing values - replace nulls with mean for numeric columns\n",
        "    # First check if there are actually missing values\n",
        "    missing_temp = df.filter(F.col(\"AverageTemperature\").isNull()).count()\n",
        "    missing_uncertainty = df.filter(F.col(\"AverageTemperatureUncertainty\").isNull()).count()\n",
        "\n",
        "    print(f\"Missing values found - AverageTemperature: {missing_temp}, AverageTemperatureUncertainty: {missing_uncertainty}\")\n",
        "\n",
        "    if missing_temp > 0 or missing_uncertainty > 0:\n",
        "        # Calculate means only if there are missing values\n",
        "        avg_temp = df.filter(F.col(\"AverageTemperature\").isNotNull()) \\\n",
        "                     .select(F.avg(\"AverageTemperature\").alias(\"avg_temp\")) \\\n",
        "                     .collect()[0][\"avg_temp\"]\n",
        "\n",
        "        avg_uncertainty = df.filter(F.col(\"AverageTemperatureUncertainty\").isNotNull()) \\\n",
        "                            .select(F.avg(\"AverageTemperatureUncertainty\").alias(\"avg_uncertainty\")) \\\n",
        "                            .collect()[0][\"avg_uncertainty\"]\n",
        "\n",
        "        print(f\"Using mean values - Temperature: {avg_temp:.2f}, Uncertainty: {avg_uncertainty:.2f}\")\n",
        "\n",
        "        # Fill missing values with means\n",
        "        df = df.fillna({\n",
        "            \"AverageTemperature\": avg_temp,\n",
        "            \"AverageTemperatureUncertainty\": avg_uncertainty\n",
        "        })\n",
        "\n",
        "    # Clean City and Country fields (trim whitespace, convert to uppercase)\n",
        "    df = df.withColumn(\"City\", F.trim(F.upper(F.col(\"City\"))))\n",
        "    df = df.withColumn(\"Country\", F.trim(F.upper(F.col(\"Country\"))))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Write to Delta Lake Bronze table with fallback to Parquet\n",
        "def write_to_bronze(df):\n",
        "    \"\"\"Write the dataframe to a Delta Lake Bronze table with partitioning.\"\"\"\n",
        "    # Create a directory for our Delta tables if it doesn't exist\n",
        "    bronze_path = \"delta_lake/bronze/weather_data\"\n",
        "\n",
        "    try:\n",
        "        # Write to Delta table partitioned by year and month\n",
        "        df.write \\\n",
        "            .format(\"delta\") \\\n",
        "            .partitionBy(\"year\", \"month\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .save(bronze_path)\n",
        "\n",
        "        print(f\"Data written to Delta Lake Bronze table at {bronze_path}\")\n",
        "        return bronze_path, \"delta\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to Delta table: {str(e)}\")\n",
        "        print(\"Falling back to Parquet format...\")\n",
        "\n",
        "        # Fallback to Parquet if Delta fails\n",
        "        parquet_path = \"parquet_data/bronze/weather_data\"\n",
        "        df.write \\\n",
        "            .format(\"parquet\") \\\n",
        "            .partitionBy(\"year\", \"month\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .save(parquet_path)\n",
        "\n",
        "        print(f\"Data written to Parquet files at {parquet_path}\")\n",
        "        return parquet_path, \"parquet\"\n",
        "\n",
        "# Create a simple logging function\n",
        "def log_process(message):\n",
        "    \"\"\"Log processing information with timestamp.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "# Main ETL process\n",
        "def run_etl_process():\n",
        "    \"\"\"Run the full ETL process.\"\"\"\n",
        "    log_process(\"Starting ETL process\")\n",
        "\n",
        "    # Make sure data is available\n",
        "    data_available = check_for_data()\n",
        "    if not data_available:\n",
        "        raise Exception(\"Data file not found. Please upload the dataset and try again.\")\n",
        "\n",
        "    # Initialize Spark\n",
        "    log_process(\"Initializing Spark session with Delta Lake support\")\n",
        "    spark = create_spark_session()\n",
        "\n",
        "    # Print Spark version info\n",
        "    spark_version = spark.version\n",
        "    log_process(f\"Using Apache Spark version: {spark_version}\")\n",
        "\n",
        "    # Define schema and load data\n",
        "    log_process(\"Loading data from CSV\")\n",
        "    schema = define_schema()\n",
        "    raw_df = load_data(spark, schema)\n",
        "\n",
        "    # Print initial stats\n",
        "    record_count = raw_df.count()\n",
        "    log_process(f\"Loaded {record_count} records\")\n",
        "    log_process(\"Sample of raw data:\")\n",
        "    raw_df.show(5)\n",
        "\n",
        "    # Data quality check - check for missing values in important columns\n",
        "    missing_data = {}\n",
        "    for col in raw_df.columns:\n",
        "        missing_count = raw_df.filter(F.col(col).isNull()).count()\n",
        "        if missing_count > 0:\n",
        "            missing_data[col] = missing_count\n",
        "\n",
        "    if missing_data:\n",
        "        log_process(\"Missing data found in the following columns:\")\n",
        "        for col, count in missing_data.items():\n",
        "            print(f\"  - {col}: {count} missing values ({(count/record_count)*100:.2f}%)\")\n",
        "    else:\n",
        "        log_process(\"No missing values found in the dataset\")\n",
        "\n",
        "    # Clean data\n",
        "    log_process(\"Cleaning and transforming data\")\n",
        "    clean_df = clean_data(raw_df)\n",
        "\n",
        "    # Print cleaned data stats\n",
        "    log_process(\"Sample of cleaned data:\")\n",
        "    clean_df.show(5)\n",
        "\n",
        "    # Write to Delta Lake/Parquet\n",
        "    log_process(\"Writing to storage\")\n",
        "    bronze_path, format_used = write_to_bronze(clean_df)\n",
        "\n",
        "    # Verify data in storage\n",
        "    log_process(f\"Verifying data in {format_used.upper()} storage\")\n",
        "    try:\n",
        "        if format_used == \"delta\":\n",
        "            bronze_df = spark.read.format(\"delta\").load(bronze_path)\n",
        "        else:\n",
        "            bronze_df = spark.read.format(\"parquet\").load(bronze_path)\n",
        "\n",
        "        bronze_count = bronze_df.count()\n",
        "        log_process(f\"Bronze table record count: {bronze_count} (using {format_used.upper()})\")\n",
        "\n",
        "        # Verify record counts match\n",
        "        if bronze_count != record_count:\n",
        "            log_process(f\"WARNING: Record count mismatch! Original: {record_count}, Bronze: {bronze_count}\")\n",
        "        else:\n",
        "            log_process(f\"Data integrity verified: {bronze_count} records successfully written\")\n",
        "\n",
        "        # Display partitioning information\n",
        "        log_process(\"Partitioning information (sample):\")\n",
        "        bronze_df.groupBy(\"year\", \"month\").count().orderBy(\"year\", \"month\").show(10)\n",
        "\n",
        "    except Exception as e:\n",
        "        log_process(f\"ERROR verifying data: {str(e)}\")\n",
        "\n",
        "    log_process(\"ETL process completed successfully\")\n",
        "\n",
        "    return spark, bronze_df\n",
        "\n",
        "# Run the ETL process if this notebook is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        spark, bronze_df = run_etl_process()\n",
        "        print(\"\\nETL process executed successfully!\")\n",
        "        print(\"You can now use 'spark' and 'bronze_df' variables in subsequent cells.\")\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"\\nERROR: ETL process failed with error: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"1. Ensure you've uploaded the globallandtemperaturesbycity.csv file\")\n",
        "        print(\"2. Check that all required packages are installed correctly\")\n",
        "        print(\"3. Restart the runtime if experiencing memory issues\")\n",
        "        print(\"4. Check for errors in the Spark UI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pi0mvKREhLtT",
        "outputId": "88c5c7ba-a872-457f-909c-e2c12f1ee0a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-06 12:23:56] Starting ETL process\n",
            "Dataset found: '/content/GlobalLandTemperaturesByCity.csv'\n",
            "[2025-05-06 12:23:56] Initializing Spark session with Delta Lake support\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(http://595cb6717a02:4040, \"/\", \"https://localhost:http://595cb6717a02:4040/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Web UI available at: http://595cb6717a02:4040\n",
            "[2025-05-06 12:23:56] Using Apache Spark version: 3.4.0\n",
            "[2025-05-06 12:23:56] Loading data from CSV\n",
            "[2025-05-06 12:24:05] Loaded 1048575 records\n",
            "[2025-05-06 12:24:05] Sample of raw data:\n",
            "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
            "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
            "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
            "|1743-11-01|             6.068|                        1.737|Århus|Denmark|  57.05N|   10.33E|\n",
            "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
            "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
            "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
            "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
            "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "[2025-05-06 12:24:21] Missing data found in the following columns:\n",
            "  - AverageTemperature: 47547 missing values (4.53%)\n",
            "  - AverageTemperatureUncertainty: 47547 missing values (4.53%)\n",
            "[2025-05-06 12:24:21] Cleaning and transforming data\n",
            "Missing values found - AverageTemperature: 47547, AverageTemperatureUncertainty: 47547\n",
            "Using mean values - Temperature: 17.93, Uncertainty: 1.03\n",
            "[2025-05-06 12:24:30] Sample of cleaned data:\n",
            "+----------+------------------+-----------------------------+-----+-------+--------+---------+----+-----+\n",
            "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|year|month|\n",
            "+----------+------------------+-----------------------------+-----+-------+--------+---------+----+-----+\n",
            "|1743-11-01|             6.068|                        1.737|ÅRHUS|DENMARK|  57.05N|   10.33E|1743|   11|\n",
            "|1743-12-01|17.928990675585677|            1.033831178548443|ÅRHUS|DENMARK|  57.05N|   10.33E|1743|   12|\n",
            "|1744-01-01|17.928990675585677|            1.033831178548443|ÅRHUS|DENMARK|  57.05N|   10.33E|1744|    1|\n",
            "|1744-02-01|17.928990675585677|            1.033831178548443|ÅRHUS|DENMARK|  57.05N|   10.33E|1744|    2|\n",
            "|1744-03-01|17.928990675585677|            1.033831178548443|ÅRHUS|DENMARK|  57.05N|   10.33E|1744|    3|\n",
            "+----------+------------------+-----------------------------+-----+-------+--------+---------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "[2025-05-06 12:24:30] Writing to storage\n",
            "Data written to Delta Lake Bronze table at delta_lake/bronze/weather_data\n",
            "[2025-05-06 12:25:56] Verifying data in DELTA storage\n",
            "[2025-05-06 12:26:00] Bronze table record count: 1048575 (using DELTA)\n",
            "[2025-05-06 12:26:00] Data integrity verified: 1048575 records successfully written\n",
            "[2025-05-06 12:26:00] Partitioning information (sample):\n",
            "+----+-----+------+\n",
            "|year|month| count|\n",
            "+----+-----+------+\n",
            "|null| null|576030|\n",
            "|1743|   11|    73|\n",
            "|1743|   12|    73|\n",
            "|1744|    1|    73|\n",
            "|1744|    2|    73|\n",
            "|1744|    3|    73|\n",
            "|1744|    4|    73|\n",
            "|1744|    5|    73|\n",
            "|1744|    6|    73|\n",
            "|1744|    7|    73|\n",
            "+----+-----+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "[2025-05-06 12:26:16] ETL process completed successfully\n",
            "\n",
            "ETL process executed successfully!\n",
            "You can now use 'spark' and 'bronze_df' variables in subsequent cells.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Module 2: EDA & Cleaning\n",
        "This notebook performs exploratory data analysis on the weather data, including summary statistics,\n",
        "distributions, missing value analysis, and creates a cleaned Delta Silver table.\n",
        "\"\"\"\n",
        "\n",
        "# Install packages if not already installed\n",
        "try:\n",
        "    import pyspark\n",
        "    import delta\n",
        "except ImportError:\n",
        "    !pip install pyspark==3.4.0 delta-spark==2.4.0 pyarrow matplotlib seaborn\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime\n",
        "from pyspark.sql.functions import col, count, isnan, when, isnull, year, month, expr\n",
        "\n",
        "# Create a Spark session with Delta Lake support\n",
        "def create_spark_session():\n",
        "    \"\"\"Create and return a Spark session with Delta Lake support.\"\"\"\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Weather Data EDA & Cleaning\") \\\n",
        "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
        "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # For Google Colab, display Spark UI link\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        display(output.serve_kernel_port_as_window(spark.sparkContext.uiWebUrl))\n",
        "        print(f\"Spark Web UI available at: {spark.sparkContext.uiWebUrl}\")\n",
        "    except:\n",
        "        print(\"Spark UI link not available\")\n",
        "\n",
        "    return spark\n",
        "\n",
        "# Create a simple logging function\n",
        "def log_process(message):\n",
        "    \"\"\"Log processing information with timestamp.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "# Check if the bronze table exists\n",
        "def load_bronze_data(spark, bronze_path=\"delta_lake/bronze/weather_data\", fallback_path=\"parquet_data/bronze/weather_data\"):\n",
        "    \"\"\"Load data from the bronze Delta table or fallback to parquet if needed.\"\"\"\n",
        "    try:\n",
        "        # Try loading from Delta first\n",
        "        log_process(\"Loading data from Delta Lake Bronze table\")\n",
        "        df = spark.read.format(\"delta\").load(bronze_path)\n",
        "        format_used = \"Delta Lake\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Delta table: {str(e)}\")\n",
        "        print(\"Falling back to Parquet format...\")\n",
        "\n",
        "        # Fallback to Parquet\n",
        "        df = spark.read.format(\"parquet\").load(fallback_path)\n",
        "        format_used = \"Parquet\"\n",
        "\n",
        "    log_process(f\"Loaded {df.count()} records from {format_used}\")\n",
        "    return df\n",
        "\n",
        "# Function to register a table for SQL queries\n",
        "def register_temp_view(df, view_name=\"weather_data\"):\n",
        "    \"\"\"Register the dataframe as a temporary view for SQL queries.\"\"\"\n",
        "    df.createOrReplaceTempView(view_name)\n",
        "    log_process(f\"Registered temp view: {view_name}\")\n",
        "    return view_name\n",
        "\n",
        "# Function to perform basic summary statistics\n",
        "def summary_statistics(df):\n",
        "    \"\"\"Calculate and display summary statistics for numeric columns.\"\"\"\n",
        "    log_process(\"Calculating summary statistics\")\n",
        "\n",
        "    # Overall summary statistics\n",
        "    print(\"\\n=== SUMMARY STATISTICS ===\")\n",
        "    df.describe().show()\n",
        "\n",
        "    # Count of records by year (sample)\n",
        "    log_process(\"Records by Year (Top 10):\")\n",
        "    df.groupBy(\"year\").count().orderBy(\"year\").show(10)\n",
        "\n",
        "    # Count of records by country (top 10)\n",
        "    log_process(\"Records by Country (Top 10):\")\n",
        "    df.groupBy(\"Country\").count().orderBy(F.desc(\"count\")).show(10)\n",
        "\n",
        "    # Temperature statistics by year (sample)\n",
        "    log_process(\"Temperature by Year (sample):\")\n",
        "    df.groupBy(\"year\") \\\n",
        "      .agg(\n",
        "          F.round(F.avg(\"AverageTemperature\"), 2).alias(\"avg_temp\"),\n",
        "          F.round(F.min(\"AverageTemperature\"), 2).alias(\"min_temp\"),\n",
        "          F.round(F.max(\"AverageTemperature\"), 2).alias(\"max_temp\"),\n",
        "          F.round(F.stddev(\"AverageTemperature\"), 2).alias(\"stddev_temp\")\n",
        "      ) \\\n",
        "      .orderBy(\"year\").show(10)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to analyze missing values\n",
        "def missing_value_analysis(df, spark):\n",
        "    \"\"\"Analyze missing values in the dataset.\"\"\"\n",
        "    log_process(\"Analyzing missing values\")\n",
        "\n",
        "    # Count missing values in each column\n",
        "    print(\"\\n=== MISSING VALUE ANALYSIS ===\")\n",
        "\n",
        "    # Create a dataframe with missing value counts and percentages\n",
        "    missing_counts = []\n",
        "    for col_name in df.columns:\n",
        "        missing_count = df.filter(col(col_name).isNull()).count()\n",
        "        total_count = df.count()\n",
        "        missing_pct = (missing_count / total_count) * 100\n",
        "        missing_counts.append((col_name, missing_count, missing_pct))\n",
        "\n",
        "    # Convert to a Spark DataFrame\n",
        "    missing_df = spark.createDataFrame(missing_counts, [\"column_name\", \"missing_count\", \"missing_percentage\"])\n",
        "    missing_df.show()\n",
        "\n",
        "    # Analyze missing temperature values by year\n",
        "    log_process(\"Missing temperature values by year (sample):\")\n",
        "    df.groupBy(\"year\") \\\n",
        "      .agg((F.sum(F.when(F.col(\"AverageTemperature\").isNull(), 1).otherwise(0))).alias(\"missing_temp_count\"),\n",
        "           F.count(\"*\").alias(\"total_count\")) \\\n",
        "      .withColumn(\"missing_percentage\", F.round((F.col(\"missing_temp_count\") / F.col(\"total_count\")) * 100, 2)) \\\n",
        "      .orderBy(\"year\").show(10)\n",
        "\n",
        "    return missing_df\n",
        "\n",
        "# Function for distribution analysis\n",
        "def distribution_analysis(df):\n",
        "    \"\"\"Analyze distributions of key columns.\"\"\"\n",
        "    log_process(\"Analyzing distributions\")\n",
        "\n",
        "    print(\"\\n=== DISTRIBUTION ANALYSIS ===\")\n",
        "\n",
        "    # Temperature distribution\n",
        "    log_process(\"Temperature Distribution:\")\n",
        "    temp_buckets = df.select(\n",
        "        F.when(col(\"AverageTemperature\") < 0, \"Below 0°C\")\n",
        "         .when(col(\"AverageTemperature\").between(0, 10), \"0-10°C\")\n",
        "         .when(col(\"AverageTemperature\").between(10, 20), \"10-20°C\")\n",
        "         .when(col(\"AverageTemperature\").between(20, 30), \"20-30°C\")\n",
        "         .when(col(\"AverageTemperature\") >= 30, \"30°C and above\")\n",
        "         .otherwise(\"Unknown\").alias(\"temp_range\")\n",
        "    )\n",
        "\n",
        "    # Count by temperature range\n",
        "    temp_buckets.groupBy(\"temp_range\").count().orderBy(\"temp_range\").show()\n",
        "\n",
        "    # Distribution by decades\n",
        "    log_process(\"Distribution by Decades:\")\n",
        "    df.withColumn(\"decade\", (F.floor(col(\"year\") / 10) * 10).cast(\"int\")) \\\n",
        "      .groupBy(\"decade\") \\\n",
        "      .count() \\\n",
        "      .orderBy(\"decade\") \\\n",
        "      .show(20)\n",
        "\n",
        "    # Show record counts by month\n",
        "    log_process(\"Record distribution by month:\")\n",
        "    df.groupBy(\"month\") \\\n",
        "      .agg(F.count(\"*\").alias(\"count\"),\n",
        "           F.round(F.avg(\"AverageTemperature\"), 2).alias(\"avg_temp\")) \\\n",
        "      .orderBy(\"month\") \\\n",
        "      .show()\n",
        "\n",
        "    # Convert small sample to Pandas for histogram\n",
        "    log_process(\"Creating temperature histogram (sample data):\")\n",
        "    sample_pd = df.select(\"AverageTemperature\").sample(False, 0.1).toPandas()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(sample_pd[\"AverageTemperature\"], bins=30, alpha=0.7)\n",
        "    plt.title(\"Distribution of Average Temperatures\")\n",
        "    plt.xlabel(\"Temperature (°C)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    return temp_buckets\n",
        "\n",
        "# Function for outlier detection\n",
        "def detect_outliers(df):\n",
        "    \"\"\"Detect outliers in the temperature data.\"\"\"\n",
        "    log_process(\"Detecting outliers\")\n",
        "\n",
        "    print(\"\\n=== OUTLIER DETECTION ===\")\n",
        "\n",
        "    # Calculate statistics for outlier detection\n",
        "    temp_stats = df.select(\n",
        "        F.mean(\"AverageTemperature\").alias(\"mean\"),\n",
        "        F.stddev(\"AverageTemperature\").alias(\"stddev\")\n",
        "    ).collect()[0]\n",
        "\n",
        "    mean_temp = temp_stats[\"mean\"]\n",
        "    stddev_temp = temp_stats[\"stddev\"]\n",
        "\n",
        "    # Define outlier thresholds (3 standard deviations)\n",
        "    lower_bound = mean_temp - 3 * stddev_temp\n",
        "    upper_bound = mean_temp + 3 * stddev_temp\n",
        "\n",
        "    log_process(f\"Temperature bounds for outliers: {lower_bound:.2f}°C to {upper_bound:.2f}°C\")\n",
        "\n",
        "    # Identify outliers\n",
        "    outliers = df.filter((col(\"AverageTemperature\") < lower_bound) |\n",
        "                         (col(\"AverageTemperature\") > upper_bound))\n",
        "\n",
        "    outlier_count = outliers.count()\n",
        "    total_count = df.count()\n",
        "    outlier_percentage = (outlier_count / total_count) * 100\n",
        "\n",
        "    log_process(f\"Found {outlier_count} outliers ({outlier_percentage:.2f}% of data)\")\n",
        "\n",
        "    # Show some example outliers\n",
        "    log_process(\"Sample of outlier records:\")\n",
        "    outliers.orderBy(col(\"AverageTemperature\")).show(5)\n",
        "    outliers.orderBy(col(\"AverageTemperature\").desc()).show(5)\n",
        "\n",
        "    return outliers\n",
        "\n",
        "# Function for SQL analysis\n",
        "def sql_analysis(spark, view_name):\n",
        "    \"\"\"Perform analysis using Spark SQL.\"\"\"\n",
        "    log_process(\"Performing SQL analysis\")\n",
        "\n",
        "    print(\"\\n=== SQL ANALYSIS ===\")\n",
        "\n",
        "    # Average temperature by continent (simplified approach)\n",
        "    log_process(\"Average temperature by continent:\")\n",
        "    continent_query = \"\"\"\n",
        "    SELECT\n",
        "        CASE\n",
        "            WHEN Country IN ('UNITED STATES', 'CANADA', 'MEXICO') THEN 'North America'\n",
        "            WHEN Country IN ('BRAZIL', 'ARGENTINA', 'CHILE', 'COLOMBIA', 'PERU', 'VENEZUELA', 'ECUADOR') THEN 'South America'\n",
        "            WHEN Country IN ('CHINA', 'INDIA', 'JAPAN', 'KOREA', 'MALAYSIA', 'THAILAND', 'VIETNAM', 'INDONESIA') THEN 'Asia'\n",
        "            WHEN Country IN ('GERMANY', 'FRANCE', 'UNITED KINGDOM', 'ITALY', 'SPAIN', 'RUSSIA', 'UKRAINE', 'POLAND', 'ROMANIA') THEN 'Europe'\n",
        "            WHEN Country IN ('AUSTRALIA', 'NEW ZEALAND') THEN 'Oceania'\n",
        "            WHEN Country IN ('EGYPT', 'SOUTH AFRICA', 'NIGERIA', 'KENYA', 'MOROCCO', 'ALGERIA', 'GHANA') THEN 'Africa'\n",
        "            ELSE 'Other'\n",
        "        END AS continent,\n",
        "        ROUND(AVG(AverageTemperature), 2) AS avg_temp,\n",
        "        COUNT(*) AS record_count\n",
        "    FROM weather_data\n",
        "    GROUP BY continent\n",
        "    ORDER BY avg_temp DESC\n",
        "    \"\"\"\n",
        "    spark.sql(continent_query).show()\n",
        "\n",
        "    # Temperature trends over time (by decade)\n",
        "    log_process(\"Temperature trends by decade:\")\n",
        "    decade_query = \"\"\"\n",
        "    SELECT\n",
        "        FLOOR(year/10)*10 AS decade,\n",
        "        ROUND(AVG(AverageTemperature), 2) AS avg_temp,\n",
        "        ROUND(MIN(AverageTemperature), 2) AS min_temp,\n",
        "        ROUND(MAX(AverageTemperature), 2) AS max_temp,\n",
        "        COUNT(*) AS record_count\n",
        "    FROM weather_data\n",
        "    GROUP BY FLOOR(year/10)*10\n",
        "    ORDER BY decade\n",
        "    \"\"\"\n",
        "    spark.sql(decade_query).show()\n",
        "\n",
        "    # Cities with highest average temperatures\n",
        "    log_process(\"Top 10 cities with highest average temperatures:\")\n",
        "    hot_cities_query = \"\"\"\n",
        "    SELECT\n",
        "        City,\n",
        "        Country,\n",
        "        ROUND(AVG(AverageTemperature), 2) AS avg_temp,\n",
        "        COUNT(*) AS record_count\n",
        "    FROM weather_data\n",
        "    GROUP BY City, Country\n",
        "    HAVING COUNT(*) > 100\n",
        "    ORDER BY avg_temp DESC\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "    spark.sql(hot_cities_query).show()\n",
        "\n",
        "    # Cities with lowest average temperatures\n",
        "    log_process(\"Top 10 cities with lowest average temperatures:\")\n",
        "    cold_cities_query = \"\"\"\n",
        "    SELECT\n",
        "        City,\n",
        "        Country,\n",
        "        ROUND(AVG(AverageTemperature), 2) AS avg_temp,\n",
        "        COUNT(*) AS record_count\n",
        "    FROM weather_data\n",
        "    GROUP BY City, Country\n",
        "    HAVING COUNT(*) > 100\n",
        "    ORDER BY avg_temp ASC\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "    spark.sql(cold_cities_query).show()\n",
        "\n",
        "    # Seasonal analysis\n",
        "    log_process(\"Seasonal temperature analysis:\")\n",
        "    season_query = \"\"\"\n",
        "    SELECT\n",
        "        CASE\n",
        "            WHEN month IN (12, 1, 2) THEN 'Winter'\n",
        "            WHEN month IN (3, 4, 5) THEN 'Spring'\n",
        "            WHEN month IN (6, 7, 8) THEN 'Summer'\n",
        "            WHEN month IN (9, 10, 11) THEN 'Fall'\n",
        "        END AS season,\n",
        "        ROUND(AVG(AverageTemperature), 2) AS avg_temp,\n",
        "        COUNT(*) AS record_count\n",
        "    FROM weather_data\n",
        "    GROUP BY\n",
        "        CASE\n",
        "            WHEN month IN (12, 1, 2) THEN 'Winter'\n",
        "            WHEN month IN (3, 4, 5) THEN 'Spring'\n",
        "            WHEN month IN (6, 7, 8) THEN 'Summer'\n",
        "            WHEN month IN (9, 10, 11) THEN 'Fall'\n",
        "        END\n",
        "    ORDER BY avg_temp DESC\n",
        "    \"\"\"\n",
        "    spark.sql(season_query).show()\n",
        "\n",
        "    return True\n",
        "\n",
        "# Function to clean and prepare data for Silver table\n",
        "def prepare_silver_data(df):\n",
        "    \"\"\"Clean and prepare data for the Silver table.\"\"\"\n",
        "    log_process(\"Preparing data for Silver table\")\n",
        "\n",
        "    # Handle null dates (filter them out)\n",
        "    cleaned_df = df.filter(col(\"dt\").isNotNull())\n",
        "\n",
        "    # Convert latitude and longitude to numeric values\n",
        "    cleaned_df = cleaned_df.withColumn(\n",
        "        \"latitude_numeric\",\n",
        "        F.when(col(\"Latitude\").endswith(\"N\"),\n",
        "              F.regexp_replace(col(\"Latitude\"), \"N\", \"\").cast(\"float\"))\n",
        "        .when(col(\"Latitude\").endswith(\"S\"),\n",
        "              F.regexp_replace(col(\"Latitude\"), \"S\", \"\").cast(\"float\") * -1)\n",
        "        .otherwise(None)\n",
        "    )\n",
        "\n",
        "    cleaned_df = cleaned_df.withColumn(\n",
        "        \"longitude_numeric\",\n",
        "        F.when(col(\"Longitude\").endswith(\"E\"),\n",
        "              F.regexp_replace(col(\"Longitude\"), \"E\", \"\").cast(\"float\"))\n",
        "        .when(col(\"Longitude\").endswith(\"W\"),\n",
        "              F.regexp_replace(col(\"Longitude\"), \"W\", \"\").cast(\"float\") * -1)\n",
        "        .otherwise(None)\n",
        "    )\n",
        "\n",
        "    # Filter out records with null or extreme temperatures\n",
        "    temp_stats = cleaned_df.select(\n",
        "        F.mean(\"AverageTemperature\").alias(\"mean\"),\n",
        "        F.stddev(\"AverageTemperature\").alias(\"stddev\")\n",
        "    ).collect()[0]\n",
        "\n",
        "    mean_temp = temp_stats[\"mean\"]\n",
        "    stddev_temp = temp_stats[\"stddev\"]\n",
        "\n",
        "    # Define outlier thresholds (3 standard deviations)\n",
        "    lower_bound = mean_temp - 3 * stddev_temp\n",
        "    upper_bound = mean_temp + 3 * stddev_temp\n",
        "\n",
        "    # Filter out extreme outliers\n",
        "    cleaned_df = cleaned_df.filter(\n",
        "        (col(\"AverageTemperature\").isNotNull()) &\n",
        "        (col(\"AverageTemperature\") >= lower_bound) &\n",
        "        (col(\"AverageTemperature\") <= upper_bound)\n",
        "    )\n",
        "\n",
        "    # Create season column\n",
        "    cleaned_df = cleaned_df.withColumn(\n",
        "        \"season\",\n",
        "        F.when(col(\"month\").isin(12, 1, 2), \"Winter\")\n",
        "        .when(col(\"month\").isin(3, 4, 5), \"Spring\")\n",
        "        .when(col(\"month\").isin(6, 7, 8), \"Summer\")\n",
        "        .when(col(\"month\").isin(9, 10, 11), \"Fall\")\n",
        "        .otherwise(\"Unknown\")\n",
        "    )\n",
        "\n",
        "    # Create hemisphere column\n",
        "    cleaned_df = cleaned_df.withColumn(\n",
        "        \"hemisphere\",\n",
        "        F.when(col(\"latitude_numeric\") >= 0, \"Northern\")\n",
        "        .when(col(\"latitude_numeric\") < 0, \"Southern\")\n",
        "        .otherwise(\"Unknown\")\n",
        "    )\n",
        "\n",
        "    # Calculate temperature normalized by global yearly average\n",
        "    window_spec = Window.partitionBy(\"year\")\n",
        "    cleaned_df = cleaned_df.withColumn(\n",
        "        \"yearly_global_avg_temp\",\n",
        "        F.avg(\"AverageTemperature\").over(window_spec)\n",
        "    )\n",
        "\n",
        "    cleaned_df = cleaned_df.withColumn(\n",
        "        \"temp_anomaly\",\n",
        "        F.round(col(\"AverageTemperature\") - col(\"yearly_global_avg_temp\"), 3)\n",
        "    )\n",
        "\n",
        "    # Final column selection and ordering for Silver table\n",
        "    silver_df = cleaned_df.select(\n",
        "        \"dt\", \"year\", \"month\", \"season\",\n",
        "        \"City\", \"Country\", \"hemisphere\",\n",
        "        \"latitude_numeric\", \"longitude_numeric\",\n",
        "        \"AverageTemperature\", \"AverageTemperatureUncertainty\",\n",
        "        \"yearly_global_avg_temp\", \"temp_anomaly\"\n",
        "    )\n",
        "\n",
        "    log_process(f\"Silver table prepared with {silver_df.count()} records\")\n",
        "    silver_df.printSchema()\n",
        "    silver_df.show(5)\n",
        "\n",
        "    return silver_df\n",
        "\n",
        "# Write to Delta Lake Silver table\n",
        "def write_to_silver(df):\n",
        "    \"\"\"Write the cleaned dataframe to a Delta Lake Silver table.\"\"\"\n",
        "    log_process(\"Writing to Delta Lake Silver table\")\n",
        "\n",
        "    # Create a directory for our Delta tables if it doesn't exist\n",
        "    silver_path = \"delta_lake/silver/cleaned_weather_data\"\n",
        "\n",
        "    try:\n",
        "        # Write to Delta table partitioned by year\n",
        "        df.write \\\n",
        "            .format(\"delta\") \\\n",
        "            .partitionBy(\"year\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .save(silver_path)\n",
        "\n",
        "        log_process(f\"Data written to Delta Lake Silver table at {silver_path}\")\n",
        "        format_used = \"delta\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to Delta table: {str(e)}\")\n",
        "        print(\"Falling back to Parquet format...\")\n",
        "\n",
        "        # Fallback to Parquet if Delta fails\n",
        "        silver_path = \"parquet_data/silver/cleaned_weather_data\"\n",
        "        df.write \\\n",
        "            .format(\"parquet\") \\\n",
        "            .partitionBy(\"year\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .save(silver_path)\n",
        "\n",
        "        log_process(f\"Data written to Parquet files at {silver_path}\")\n",
        "        format_used = \"parquet\"\n",
        "\n",
        "    return silver_path, format_used\n",
        "\n",
        "# Main process\n",
        "def run_eda_cleaning_process():\n",
        "    \"\"\"Run the full EDA and Cleaning process.\"\"\"\n",
        "    log_process(\"Starting EDA and Cleaning process\")\n",
        "\n",
        "    # Initialize Spark\n",
        "    spark = create_spark_session()\n",
        "\n",
        "    # Load data from Bronze table\n",
        "    bronze_df = load_bronze_data(spark)\n",
        "\n",
        "    # Register view for SQL analysis\n",
        "    view_name = register_temp_view(bronze_df)\n",
        "\n",
        "    # Perform EDA\n",
        "    summary_statistics(bronze_df)\n",
        "    missing_df = missing_value_analysis(bronze_df, spark)\n",
        "    temp_dist = distribution_analysis(bronze_df)\n",
        "    outliers = detect_outliers(bronze_df)\n",
        "    sql_analysis(spark, view_name)\n",
        "\n",
        "    # Clean and prepare data for Silver table\n",
        "    silver_df = prepare_silver_data(bronze_df)\n",
        "\n",
        "    # Write to Silver table\n",
        "    silver_path, format_used = write_to_silver(silver_df)\n",
        "\n",
        "    # Verify Silver table\n",
        "    log_process(f\"Verifying {format_used} Silver table at {silver_path}\")\n",
        "    if format_used == \"delta\":\n",
        "        verified_df = spark.read.format(\"delta\").load(silver_path)\n",
        "    else:\n",
        "        verified_df = spark.read.format(\"parquet\").load(silver_path)\n",
        "\n",
        "    log_process(f\"Silver table record count: {verified_df.count()}\")\n",
        "    log_process(\"Sample of Silver table:\")\n",
        "    verified_df.show(5)\n",
        "\n",
        "    log_process(\"EDA and Cleaning process completed successfully\")\n",
        "\n",
        "    return spark, verified_df\n",
        "\n",
        "# Run the process if this notebook is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        spark, silver_df = run_eda_cleaning_process()\n",
        "        print(\"\\nEDA and Cleaning process executed successfully!\")\n",
        "        print(\"You can now use 'spark' and 'silver_df' variables in subsequent cells.\")\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"\\nERROR: EDA and Cleaning process failed with error: {str(e)}\")\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PsNq-IEfkAmY",
        "outputId": "3a5e7e58-3f1c-47af-e0f4-462e09ef2b14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-06 13:04:37] Starting EDA and Cleaning process\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(http://595cb6717a02:4040, \"/\", \"https://localhost:http://595cb6717a02:4040/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Web UI available at: http://595cb6717a02:4040\n",
            "[2025-05-06 13:04:37] Loading data from Delta Lake Bronze table\n",
            "[2025-05-06 13:04:38] Loaded 1048575 records from Delta Lake\n",
            "[2025-05-06 13:04:38] Registered temp view: weather_data\n",
            "[2025-05-06 13:04:38] Calculating summary statistics\n",
            "\n",
            "=== SUMMARY STATISTICS ===\n",
            "+-------+------------------+-----------------------------+--------+-----------+--------+---------+------------------+------------------+\n",
            "|summary|AverageTemperature|AverageTemperatureUncertainty|    City|    Country|Latitude|Longitude|              year|             month|\n",
            "+-------+------------------+-----------------------------+--------+-----------+--------+---------+------------------+------------------+\n",
            "|  count|           1048575|                      1048575| 1048575|    1048575| 1048575|  1048575|            472545|            472545|\n",
            "|   mean|17.928990675585577|           1.0338311785484533|    null|       null|    null|     null|1845.5741252155879| 6.503645155487837|\n",
            "| stddev|10.121254583179862|           1.0777054236012762|    null|       null|    null|     null| 37.89577943761585|3.4526056320521783|\n",
            "|    min|           -31.874|                        0.036|A CORUÑA|AFGHANISTAN|   0.80N|    0.00W|              1743|                 1|\n",
            "|    max|            39.156|                        15.03|  ÜRÜMQI|      YEMEN|   8.84S|   99.91E|              1899|                12|\n",
            "+-------+------------------+-----------------------------+--------+-----------+--------+---------+------------------+------------------+\n",
            "\n",
            "[2025-05-06 13:05:10] Records by Year (Top 10):\n",
            "+----+------+\n",
            "|year| count|\n",
            "+----+------+\n",
            "|null|576030|\n",
            "|1743|   146|\n",
            "|1744|   876|\n",
            "|1745|   876|\n",
            "|1746|   876|\n",
            "|1747|   876|\n",
            "|1748|   876|\n",
            "|1749|   876|\n",
            "|1750|   876|\n",
            "|1751|   876|\n",
            "+----+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "[2025-05-06 13:05:17] Records by Country (Top 10):\n",
            "+-------------+------+\n",
            "|      Country| count|\n",
            "+-------------+------+\n",
            "|        INDIA|223750|\n",
            "|UNITED STATES| 63721|\n",
            "|       BRAZIL| 60852|\n",
            "|        CHINA| 57703|\n",
            "|       RUSSIA| 45866|\n",
            "|    INDONESIA| 44449|\n",
            "|        SPAIN| 35409|\n",
            "|       TURKEY| 29100|\n",
            "|      NIGERIA| 24609|\n",
            "|        JAPAN| 24606|\n",
            "+-------------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "[2025-05-06 13:05:25] Temperature by Year (sample):\n",
            "+----+--------+--------+--------+-----------+\n",
            "|year|avg_temp|min_temp|max_temp|stddev_temp|\n",
            "+----+--------+--------+--------+-----------+\n",
            "|null|   18.91|  -31.87|   39.16|      10.08|\n",
            "|1743|    11.7|   -7.73|   17.93|       6.99|\n",
            "|1744|   13.31|  -13.11|   27.37|       6.41|\n",
            "|1745|   12.72|  -16.63|   17.93|       8.17|\n",
            "|1746|   17.93|   17.93|   17.93|        0.0|\n",
            "|1747|   17.93|   17.93|   17.93|        0.0|\n",
            "|1748|   17.93|   17.93|   17.93|        0.0|\n",
            "|1749|   17.93|   17.93|   17.93|        0.0|\n",
            "|1750|   11.45|  -11.85|   29.61|       7.75|\n",
            "|1751|   13.32|  -15.01|   28.61|       7.76|\n",
            "+----+--------+--------+--------+-----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "[2025-05-06 13:05:33] Analyzing missing values\n",
            "\n",
            "=== MISSING VALUE ANALYSIS ===\n",
            "+--------------------+-------------+------------------+\n",
            "|         column_name|missing_count|missing_percentage|\n",
            "+--------------------+-------------+------------------+\n",
            "|                  dt|       576030| 54.93455403762248|\n",
            "|  AverageTemperature|            0|               0.0|\n",
            "|AverageTemperatur...|            0|               0.0|\n",
            "|                City|            0|               0.0|\n",
            "|             Country|            0|               0.0|\n",
            "|            Latitude|            0|               0.0|\n",
            "|           Longitude|            0|               0.0|\n",
            "|                year|       576030| 54.93455403762248|\n",
            "|               month|       576030| 54.93455403762248|\n",
            "+--------------------+-------------+------------------+\n",
            "\n",
            "[2025-05-06 13:05:54] Missing temperature values by year (sample):\n",
            "+----+------------------+-----------+------------------+\n",
            "|year|missing_temp_count|total_count|missing_percentage|\n",
            "+----+------------------+-----------+------------------+\n",
            "|null|                 0|     576030|               0.0|\n",
            "|1743|                 0|        146|               0.0|\n",
            "|1744|                 0|        876|               0.0|\n",
            "|1745|                 0|        876|               0.0|\n",
            "|1746|                 0|        876|               0.0|\n",
            "|1747|                 0|        876|               0.0|\n",
            "|1748|                 0|        876|               0.0|\n",
            "|1749|                 0|        876|               0.0|\n",
            "|1750|                 0|        876|               0.0|\n",
            "|1751|                 0|        876|               0.0|\n",
            "+----+------------------+-----------+------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "[2025-05-06 13:06:04] Analyzing distributions\n",
            "\n",
            "=== DISTRIBUTION ANALYSIS ===\n",
            "[2025-05-06 13:06:04] Temperature Distribution:\n",
            "+--------------+------+\n",
            "|    temp_range| count|\n",
            "+--------------+------+\n",
            "|        0-10°C|145722|\n",
            "|       10-20°C|320806|\n",
            "|       20-30°C|468602|\n",
            "|30°C and above| 48571|\n",
            "|     Below 0°C| 64874|\n",
            "+--------------+------+\n",
            "\n",
            "[2025-05-06 13:06:12] Distribution by Decades:\n",
            "+------+------+\n",
            "|decade| count|\n",
            "+------+------+\n",
            "|  null|576030|\n",
            "|  1740|  5402|\n",
            "|  1750|  9514|\n",
            "|  1760|  9960|\n",
            "|  1770| 10099|\n",
            "|  1780| 11120|\n",
            "|  1790| 16448|\n",
            "|  1800| 23145|\n",
            "|  1810| 24740|\n",
            "|  1820| 32344|\n",
            "|  1830| 38286|\n",
            "|  1840| 42284|\n",
            "|  1850| 47302|\n",
            "|  1860| 50173|\n",
            "|  1870| 50448|\n",
            "|  1880| 50640|\n",
            "|  1890| 50640|\n",
            "+------+------+\n",
            "\n",
            "[2025-05-06 13:06:19] Record distribution by month:\n",
            "+-----+------+--------+\n",
            "|month| count|avg_temp|\n",
            "+-----+------+--------+\n",
            "| null|576030|   18.91|\n",
            "|    1| 39331|    9.51|\n",
            "|    2| 39331|   10.99|\n",
            "|    3| 39341|   13.93|\n",
            "|    4| 39346|   17.27|\n",
            "|    5| 39362|   20.22|\n",
            "|    6| 39363|   22.07|\n",
            "|    7| 39364|   22.84|\n",
            "|    8| 39385|   22.37|\n",
            "|    9| 39386|   20.36|\n",
            "|   10| 39393|   17.17|\n",
            "|   11| 39471|   13.59|\n",
            "|   12| 39472|   10.47|\n",
            "+-----+------+--------+\n",
            "\n",
            "[2025-05-06 13:06:28] Creating temperature histogram (sample data):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-06 13:06:35] Detecting outliers\n",
            "\n",
            "=== OUTLIER DETECTION ===\n",
            "[2025-05-06 13:06:42] Temperature bounds for outliers: -12.43°C to 48.29°C\n",
            "[2025-05-06 13:06:47] Found 12076 outliers (1.15% of data)\n",
            "[2025-05-06 13:06:47] Sample of outlier records:\n",
            "+----------+------------------+-----------------------------+---------------+-------+--------+---------+----+-----+\n",
            "|        dt|AverageTemperature|AverageTemperatureUncertainty|           City|Country|Latitude|Longitude|year|month|\n",
            "+----------+------------------+-----------------------------+---------------+-------+--------+---------+----+-----+\n",
            "|      null|           -31.874|                         0.46|BLAGOVESHCHENSK| RUSSIA|  50.63N|  128.03E|null| null|\n",
            "|1861-01-01|           -31.473|                        5.872|BLAGOVESHCHENSK| RUSSIA|  50.63N|  128.03E|1861|    1|\n",
            "|      null|           -31.312|                         0.63|        BARNAUL| RUSSIA|  53.84N|   83.18E|null| null|\n",
            "|1893-01-01|           -31.138|                        1.168|         ABAKAN| RUSSIA|  53.84N|   91.36E|1893|    1|\n",
            "|      null|           -30.997|                        0.692|BLAGOVESHCHENSK| RUSSIA|  50.63N|  128.03E|null| null|\n",
            "+----------+------------------+-----------------------------+---------------+-------+--------+---------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+------------------+-----------------------------+---------+-------+--------+---------+----+-----+\n",
            "|        dt|AverageTemperature|AverageTemperatureUncertainty|     City|Country|Latitude|Longitude|year|month|\n",
            "+----------+------------------+-----------------------------+---------+-------+--------+---------+----+-----+\n",
            "|1890-11-01|           -12.435|                        2.008|    BIYSK| RUSSIA|  52.24N|   84.09E|1890|   11|\n",
            "|1828-02-01|           -12.436|                        1.454| BELGOROD| RUSSIA|  50.63N|   36.76E|1828|    2|\n",
            "|1845-02-01|           -12.437|                        3.326|BIALYSTOK| POLAND|  53.84N|   23.18E|1845|    2|\n",
            "|1866-12-01|           -12.437|                        3.161|   ABAKAN| RUSSIA|  53.84N|   91.36E|1866|   12|\n",
            "|1862-01-01|           -12.438|                         2.24|   BAIYIN|  CHINA|  36.17N|  103.43E|1862|    1|\n",
            "+----------+------------------+-----------------------------+---------+-------+--------+---------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "[2025-05-06 13:06:56] Performing SQL analysis\n",
            "\n",
            "=== SQL ANALYSIS ===\n",
            "[2025-05-06 13:06:56] Average temperature by continent:\n",
            "+-------------+--------+------------+\n",
            "|    continent|avg_temp|record_count|\n",
            "+-------------+--------+------------+\n",
            "|South America|   22.05|       97270|\n",
            "|         Asia|   21.98|      368734|\n",
            "|       Africa|    21.8|       58706|\n",
            "|        Other|   17.23|      267403|\n",
            "|      Oceania|   14.93|        6075|\n",
            "|North America|   13.27|       77930|\n",
            "|       Europe|    8.92|      172457|\n",
            "+-------------+--------+------------+\n",
            "\n",
            "[2025-05-06 13:07:06] Temperature trends by decade:\n",
            "+------+--------+--------+--------+------------+\n",
            "|decade|avg_temp|min_temp|max_temp|record_count|\n",
            "+------+--------+--------+--------+------------+\n",
            "|  null|   18.91|  -31.87|   39.16|      576030|\n",
            "|  1740|   16.17|  -16.63|   27.37|        5402|\n",
            "|  1750|   10.54|  -20.54|   31.06|        9514|\n",
            "|  1760|   10.09|  -20.25|   32.53|        9960|\n",
            "|  1770|    10.4|  -19.37|    29.4|       10099|\n",
            "|  1780|   10.48|  -21.34|   30.55|       11120|\n",
            "|  1790|   15.08|  -19.26|   36.07|       16448|\n",
            "|  1800|   17.29|  -21.54|   36.51|       23145|\n",
            "|  1810|   16.22|  -22.09|   35.67|       24740|\n",
            "|  1820|    17.0|  -27.98|   35.87|       32344|\n",
            "|  1830|   16.53|  -29.54|   36.55|       38286|\n",
            "|  1840|   16.83|  -30.52|   37.98|       42284|\n",
            "|  1850|   17.54|  -27.86|   36.96|       47302|\n",
            "|  1860|   17.02|  -31.47|   37.62|       50173|\n",
            "|  1870|   18.16|  -30.64|   37.78|       50448|\n",
            "|  1880|   18.21|  -30.18|   36.89|       50640|\n",
            "|  1890|   18.39|  -31.14|   37.26|       50640|\n",
            "+------+--------+--------+--------+------------+\n",
            "\n",
            "[2025-05-06 13:07:14] Top 10 cities with highest average temperatures:\n",
            "+--------------+------------+--------+------------+\n",
            "|          City|     Country|avg_temp|record_count|\n",
            "+--------------+------------+--------+------------+\n",
            "|      AMBATTUR|       INDIA|    28.0|        2613|\n",
            "|         AVADI|       INDIA|    28.0|        2613|\n",
            "|       ALANDUR|       INDIA|    28.0|        2613|\n",
            "|          BAMA|     NIGERIA|   27.08|        1893|\n",
            "|    BHIMAVARAM|       INDIA|   27.03|        2613|\n",
            "|BOBO DIOULASSO|BURKINA FASO|   26.99|        1977|\n",
            "|       BACOLOD| PHILIPPINES|   26.93|        1929|\n",
            "|         AMBUR|       INDIA|   26.91|        2613|\n",
            "|       BELLARY|       INDIA|   26.88|        2613|\n",
            "|         ADONI|       INDIA|   26.88|        2613|\n",
            "+--------------+------------+--------+------------+\n",
            "\n",
            "[2025-05-06 13:07:24] Top 10 cities with lowest average temperatures:\n",
            "+---------------+-------------+--------+------------+\n",
            "|           City|      Country|avg_temp|record_count|\n",
            "+---------------+-------------+--------+------------+\n",
            "|        ANGARSK|       RUSSIA|   -1.81|        2318|\n",
            "|      ANCHORAGE|UNITED STATES|   -1.61|        2229|\n",
            "|        ACHINSK|       RUSSIA|   -1.04|        2318|\n",
            "|BLAGOVESHCHENSK|       RUSSIA|   -0.71|        2318|\n",
            "|         ABAKAN|       RUSSIA|   -0.54|        2318|\n",
            "|          ALTAY|        CHINA|   -0.07|        2318|\n",
            "|        BARNAUL|       RUSSIA|    0.45|        2318|\n",
            "|    ARKHANGELSK|       RUSSIA|    0.72|        3239|\n",
            "|        ÖSKEMEN|   KAZAKHSTAN|    1.32|        2318|\n",
            "|          BEIAN|        CHINA|    1.42|        2318|\n",
            "+---------------+-------------+--------+------------+\n",
            "\n",
            "[2025-05-06 13:07:34] Seasonal temperature analysis:\n",
            "+------+--------+------------+\n",
            "|season|avg_temp|record_count|\n",
            "+------+--------+------------+\n",
            "|Summer|   22.43|      118112|\n",
            "|  null|   18.91|      576030|\n",
            "|Spring|   17.14|      118049|\n",
            "|  Fall|   17.04|      118250|\n",
            "|Winter|   10.32|      118134|\n",
            "+------+--------+------------+\n",
            "\n",
            "[2025-05-06 13:07:41] Preparing data for Silver table\n",
            "[2025-05-06 13:08:04] Silver table prepared with 467690 records\n",
            "root\n",
            " |-- dt: date (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- season: string (nullable = false)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- hemisphere: string (nullable = false)\n",
            " |-- latitude_numeric: float (nullable = true)\n",
            " |-- longitude_numeric: float (nullable = true)\n",
            " |-- AverageTemperature: double (nullable = true)\n",
            " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
            " |-- yearly_global_avg_temp: double (nullable = true)\n",
            " |-- temp_anomaly: double (nullable = true)\n",
            "\n",
            "+----------+----+-----+------+----------+--------------------+----------+----------------+-----------------+------------------+-----------------------------+----------------------+------------+\n",
            "|        dt|year|month|season|      City|             Country|hemisphere|latitude_numeric|longitude_numeric|AverageTemperature|AverageTemperatureUncertainty|yearly_global_avg_temp|temp_anomaly|\n",
            "+----------+----+-----+------+----------+--------------------+----------+----------------+-----------------+------------------+-----------------------------+----------------------+------------+\n",
            "|1743-11-01|1743|   11|  Fall|  BALAKOVO|              RUSSIA|  Northern|           52.24|             47.3|             -4.17|                        2.555|    11.700769310395584|     -15.871|\n",
            "|1743-11-01|1743|   11|  Fall|BALASHIKHA|              RUSSIA|  Northern|           55.45|            36.85|             -2.57|                        2.037|    11.700769310395584|     -14.271|\n",
            "|1743-11-01|1743|   11|  Fall|     BALTI|             MOLDOVA|  Northern|           47.42|            27.24|             3.039|                        2.172|    11.700769310395584|      -8.662|\n",
            "|1743-11-01|1743|   11|  Fall| BALTIMORE|       UNITED STATES|  Northern|           39.38|           -76.99|             5.339|                        1.828|    11.700769310395584|      -6.362|\n",
            "|1743-11-01|1743|   11|  Fall|BANJA LUKA|BOSNIA AND HERZEG...|  Northern|            44.2|            17.89|             4.983|                        2.041|    11.700769310395584|      -6.718|\n",
            "+----------+----+-----+------+----------+--------------------+----------+----------------+-----------------+------------------+-----------------------------+----------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "[2025-05-06 13:08:19] Writing to Delta Lake Silver table\n",
            "[2025-05-06 13:08:53] Data written to Delta Lake Silver table at delta_lake/silver/cleaned_weather_data\n",
            "[2025-05-06 13:08:53] Verifying delta Silver table at delta_lake/silver/cleaned_weather_data\n",
            "[2025-05-06 13:08:56] Silver table record count: 467690\n",
            "[2025-05-06 13:08:56] Sample of Silver table:\n",
            "+----------+----+-----+------+--------+-------+----------+----------------+-----------------+------------------+-----------------------------+----------------------+------------+\n",
            "|        dt|year|month|season|    City|Country|hemisphere|latitude_numeric|longitude_numeric|AverageTemperature|AverageTemperatureUncertainty|yearly_global_avg_temp|temp_anomaly|\n",
            "+----------+----+-----+------+--------+-------+----------+----------------+-----------------+------------------+-----------------------------+----------------------+------------+\n",
            "|1773-12-01|1773|   12|Winter|   ÅRHUS|DENMARK|  Northern|           57.05|            10.33|             1.644|                        6.646|    10.633853245837999|       -8.99|\n",
            "|1773-12-01|1773|   12|Winter|   ÇORLU| TURKEY|  Northern|           40.99|            27.69|              9.41|                        4.342|    10.633853245837999|      -1.224|\n",
            "|1773-12-01|1773|   12|Winter|   ÇORUM| TURKEY|  Northern|           40.99|            34.08|             4.173|                        3.958|    10.633853245837999|      -6.461|\n",
            "|1773-12-01|1773|   12|Winter|A CORUÑA|  SPAIN|  Northern|           42.59|            -8.73|             9.974|                        4.294|    10.633853245837999|       -0.66|\n",
            "|1773-12-01|1773|   12|Winter|  AACHEN|GERMANY|  Northern|           50.63|             6.34|             3.372|                        6.685|    10.633853245837999|      -7.262|\n",
            "+----------+----+-----+------+--------+-------+----------+----------------+-----------------+------------------+-----------------------------+----------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "[2025-05-06 13:08:58] EDA and Cleaning process completed successfully\n",
            "\n",
            "EDA and Cleaning process executed successfully!\n",
            "You can now use 'spark' and 'silver_df' variables in subsequent cells.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Module 3: Feature Engineering\n",
        "This notebook performs feature engineering on the cleaned weather data,\n",
        "using PySpark transformers to prepare the data for machine learning.\n",
        "\"\"\"\n",
        "\n",
        "# Install packages if not already installed\n",
        "try:\n",
        "    import pyspark\n",
        "    import delta\n",
        "except ImportError:\n",
        "    !pip install pyspark==3.4.0 delta-spark==2.4.0 pyarrow matplotlib seaborn\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Bucketizer, SQLTransformer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Create a Spark session with Delta Lake support\n",
        "def create_spark_session():\n",
        "    \"\"\"Create and return a Spark session with Delta Lake support.\"\"\"\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Weather Data Feature Engineering\") \\\n",
        "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
        "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # For Google Colab, display Spark UI link\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        display(output.serve_kernel_port_as_window(spark.sparkContext.uiWebUrl))\n",
        "        print(f\"Spark Web UI available at: {spark.sparkContext.uiWebUrl}\")\n",
        "    except:\n",
        "        print(\"Spark UI link not available\")\n",
        "\n",
        "    return spark\n",
        "\n",
        "# Create a simple logging function\n",
        "def log_process(message):\n",
        "    \"\"\"Log processing information with timestamp.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "# Check if the silver table exists and load data\n",
        "def load_silver_data(spark, silver_path=\"delta_lake/silver/cleaned_weather_data\", fallback_path=\"parquet_data/silver/cleaned_weather_data\"):\n",
        "    \"\"\"Load data from the silver Delta table or fallback to parquet if needed.\"\"\"\n",
        "    try:\n",
        "        # Try loading from Delta first\n",
        "        log_process(\"Loading data from Delta Lake Silver table\")\n",
        "        df = spark.read.format(\"delta\").load(silver_path)\n",
        "        format_used = \"Delta Lake\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Delta table: {str(e)}\")\n",
        "        print(\"Falling back to Parquet format...\")\n",
        "\n",
        "        # Fallback to Parquet\n",
        "        df = spark.read.format(\"parquet\").load(fallback_path)\n",
        "        format_used = \"Parquet\"\n",
        "\n",
        "    log_process(f\"Loaded {df.count()} records from {format_used}\")\n",
        "    return df\n",
        "\n",
        "# Function to create temporal features\n",
        "def create_temporal_features(df):\n",
        "    \"\"\"Create temporal features from the date column.\"\"\"\n",
        "    log_process(\"Creating temporal features\")\n",
        "\n",
        "    # Create day of month, day of week, day of year features\n",
        "    df = df.withColumn(\"day_of_month\", F.dayofmonth(F.col(\"dt\")))\n",
        "    df = df.withColumn(\"day_of_week\", F.dayofweek(F.col(\"dt\")))\n",
        "    df = df.withColumn(\"day_of_year\", F.dayofyear(F.col(\"dt\")))\n",
        "\n",
        "    # Create week of year feature\n",
        "    df = df.withColumn(\"week_of_year\", F.weekofyear(F.col(\"dt\")))\n",
        "\n",
        "    # Create quarter feature\n",
        "    df = df.withColumn(\"quarter\", F.quarter(F.col(\"dt\")))\n",
        "\n",
        "    # Create month_sin and month_cos for cyclical encoding of month\n",
        "    df = df.withColumn(\"month_sin\", F.sin(F.col(\"month\") * 2 * 3.14159 / 12))\n",
        "    df = df.withColumn(\"month_cos\", F.cos(F.col(\"month\") * 2 * 3.14159 / 12))\n",
        "\n",
        "    # Create decade feature\n",
        "    df = df.withColumn(\"decade\", (F.floor(F.col(\"year\") / 10) * 10).cast(\"int\"))\n",
        "\n",
        "    # Create century feature\n",
        "    df = df.withColumn(\"century\", (F.floor(F.col(\"year\") / 100) + 1).cast(\"int\"))\n",
        "\n",
        "    # Create is_leap_year feature\n",
        "    df = df.withColumn(\"is_leap_year\",\n",
        "                      ((F.col(\"year\") % 4 == 0) &\n",
        "                       ((F.col(\"year\") % 100 != 0) | (F.col(\"year\") % 400 == 0))).cast(\"int\"))\n",
        "\n",
        "    log_process(\"Temporal features created\")\n",
        "    return df\n",
        "\n",
        "# Function to create location-based features\n",
        "def create_location_features(df):\n",
        "    \"\"\"Create location-based features.\"\"\"\n",
        "    log_process(\"Creating location-based features\")\n",
        "\n",
        "    # Create climate zone features based on latitude\n",
        "    df = df.withColumn(\n",
        "        \"climate_zone\",\n",
        "        F.when((F.col(\"latitude_numeric\") >= -23.5) & (F.col(\"latitude_numeric\") <= 23.5), \"Tropical\")\n",
        "         .when(((F.col(\"latitude_numeric\") > 23.5) & (F.col(\"latitude_numeric\") <= 66.5)) |\n",
        "               ((F.col(\"latitude_numeric\") >= -66.5) & (F.col(\"latitude_numeric\") < -23.5)), \"Temperate\")\n",
        "         .when((F.col(\"latitude_numeric\") > 66.5) | (F.col(\"latitude_numeric\") < -66.5), \"Polar\")\n",
        "         .otherwise(\"Unknown\")\n",
        "    )\n",
        "\n",
        "    # Create distance from equator feature\n",
        "    df = df.withColumn(\"distance_from_equator\", F.abs(F.col(\"latitude_numeric\")))\n",
        "\n",
        "    # Create is_coastal feature (simplified approach - using longitude)\n",
        "    # This is a simplification; a more accurate approach would use coastline data\n",
        "    df = df.withColumn(\n",
        "        \"longitude_abs\",\n",
        "        F.abs(F.col(\"longitude_numeric\"))\n",
        "    )\n",
        "\n",
        "    # Binning distance from equator\n",
        "    df = df.withColumn(\n",
        "        \"latitude_bin\",\n",
        "        F.when(F.col(\"latitude_numeric\").between(-90, -60), \"Antarctic\")\n",
        "         .when(F.col(\"latitude_numeric\").between(-60, -30), \"Southern Temperate\")\n",
        "         .when(F.col(\"latitude_numeric\").between(-30, 0), \"Southern Subtropical\")\n",
        "         .when(F.col(\"latitude_numeric\").between(0, 30), \"Northern Subtropical\")\n",
        "         .when(F.col(\"latitude_numeric\").between(30, 60), \"Northern Temperate\")\n",
        "         .when(F.col(\"latitude_numeric\").between(60, 90), \"Arctic\")\n",
        "         .otherwise(\"Unknown\")\n",
        "    )\n",
        "\n",
        "    log_process(\"Location features created\")\n",
        "    return df\n",
        "\n",
        "# Function to create temperature-derived features\n",
        "def create_temperature_features(df):\n",
        "    \"\"\"Create derived features from temperature data.\"\"\"\n",
        "    log_process(\"Creating temperature-derived features\")\n",
        "\n",
        "    # Create temperature volatility features using window functions\n",
        "    # Yearly temperature range by city\n",
        "    window_city_year = Window.partitionBy(\"City\", \"Country\", \"year\")\n",
        "\n",
        "    df = df.withColumn(\"yearly_city_max_temp\", F.max(\"AverageTemperature\").over(window_city_year))\n",
        "    df = df.withColumn(\"yearly_city_min_temp\", F.min(\"AverageTemperature\").over(window_city_year))\n",
        "    df = df.withColumn(\"yearly_city_temp_range\",\n",
        "                      F.col(\"yearly_city_max_temp\") - F.col(\"yearly_city_min_temp\"))\n",
        "\n",
        "    # Create temperature bins\n",
        "    df = df.withColumn(\n",
        "        \"temp_category\",\n",
        "        F.when(F.col(\"AverageTemperature\") < 0, \"Freezing\")\n",
        "         .when(F.col(\"AverageTemperature\").between(0, 10), \"Cold\")\n",
        "         .when(F.col(\"AverageTemperature\").between(10, 20), \"Mild\")\n",
        "         .when(F.col(\"AverageTemperature\").between(20, 30), \"Warm\")\n",
        "         .when(F.col(\"AverageTemperature\") >= 30, \"Hot\")\n",
        "         .otherwise(\"Unknown\")\n",
        "    )\n",
        "\n",
        "    # Create temperature z-score (standardized temperature)\n",
        "    window_global = Window.partitionBy()\n",
        "    df = df.withColumn(\"global_temp_mean\", F.mean(\"AverageTemperature\").over(window_global))\n",
        "    df = df.withColumn(\"global_temp_stddev\", F.stddev(\"AverageTemperature\").over(window_global))\n",
        "    df = df.withColumn(\"temp_zscore\",\n",
        "                      (F.col(\"AverageTemperature\") - F.col(\"global_temp_mean\")) /\n",
        "                      F.col(\"global_temp_stddev\"))\n",
        "\n",
        "    # Create seasonal features - temperature difference from seasonal average\n",
        "    window_season = Window.partitionBy(\"season\", \"hemisphere\")\n",
        "    df = df.withColumn(\"season_avg_temp\", F.avg(\"AverageTemperature\").over(window_season))\n",
        "    df = df.withColumn(\"temp_diff_from_season_avg\",\n",
        "                      F.col(\"AverageTemperature\") - F.col(\"season_avg_temp\"))\n",
        "\n",
        "    # Moving average temperature (5-year)\n",
        "    window_5yr = Window.partitionBy(\"City\", \"Country\") \\\n",
        "                       .orderBy(\"year\") \\\n",
        "                       .rangeBetween(-2, 2)  # 5-year window centered on current year\n",
        "\n",
        "    df = df.withColumn(\"temp_5yr_avg\", F.avg(\"AverageTemperature\").over(window_5yr))\n",
        "\n",
        "    # Temperature trend (difference from 5-year average)\n",
        "    df = df.withColumn(\"temp_trend\", F.col(\"AverageTemperature\") - F.col(\"temp_5yr_avg\"))\n",
        "\n",
        "    # Temperature volatility (standard deviation over 5-year period)\n",
        "    df = df.withColumn(\"temp_5yr_stddev\", F.stddev(\"AverageTemperature\").over(window_5yr))\n",
        "\n",
        "    log_process(\"Temperature features created\")\n",
        "    return df\n",
        "\n",
        "# Apply transformers to prepare for ML\n",
        "def apply_transformers(df):\n",
        "    \"\"\"Apply PySpark ML transformers to prepare data for machine learning.\"\"\"\n",
        "    log_process(\"Applying PySpark transformers\")\n",
        "\n",
        "    # List columns to process with transformers\n",
        "    categorical_cols = [\"season\", \"hemisphere\", \"climate_zone\", \"temp_category\", \"latitude_bin\"]\n",
        "    numerical_cols = [\n",
        "        \"latitude_numeric\", \"longitude_numeric\", \"month_sin\", \"month_cos\",\n",
        "        \"distance_from_equator\", \"temp_anomaly\", \"temp_zscore\",\n",
        "        \"temp_diff_from_season_avg\", \"temp_trend\", \"temp_5yr_stddev\"\n",
        "    ]\n",
        "\n",
        "    # Initialize the ML pipeline stages\n",
        "    stages = []\n",
        "\n",
        "    # String Indexer for categorical features\n",
        "    for col in categorical_cols:\n",
        "        indexer = StringIndexer(\n",
        "            inputCol=col,\n",
        "            outputCol=f\"{col}_idx\",\n",
        "            handleInvalid=\"skip\",  # Handle unseen labels\n",
        "            stringOrderType=\"alphabetAsc\" # Add this to fix the error\n",
        "        )\n",
        "        stages.append(indexer)\n",
        "\n",
        "    # One-hot encoding for indexed categories\n",
        "    indexed_cats = [f\"{col}_idx\" for col in categorical_cols]\n",
        "    encoder = OneHotEncoder(\n",
        "        inputCols=indexed_cats,\n",
        "        outputCols=[f\"{col}_vec\" for col in categorical_cols]\n",
        "    )\n",
        "    stages.append(encoder)\n",
        "\n",
        "    # SQL Transformer for custom transformations\n",
        "    sql_transformer = SQLTransformer(\n",
        "        statement=\"\"\"\n",
        "        SELECT *,\n",
        "            (latitude_numeric * longitude_numeric) AS lat_long_interaction,\n",
        "            POWER(latitude_numeric, 2) AS latitude_squared,\n",
        "            POWER(longitude_numeric, 2) AS longitude_squared,\n",
        "            (AverageTemperature * distance_from_equator) AS temp_lat_interaction\n",
        "        FROM __THIS__\n",
        "        \"\"\"\n",
        "    )\n",
        "    stages.append(sql_transformer)\n",
        "\n",
        "    # Add the new SQL-generated columns to our numerical features\n",
        "    numerical_cols.extend([\"lat_long_interaction\", \"latitude_squared\", \"longitude_squared\", \"temp_lat_interaction\"])\n",
        "\n",
        "    # Temperature Bucketizer\n",
        "    temp_bucketizer = Bucketizer(\n",
        "        splits=[-float(\"inf\"), 0, 10, 20, 30, float(\"inf\")],\n",
        "        inputCol=\"AverageTemperature\",\n",
        "        outputCol=\"temp_buckets\"\n",
        "    )\n",
        "    stages.append(temp_bucketizer)\n",
        "\n",
        "    # Vector Assembler for feature vector\n",
        "    assembler_inputs = numerical_cols + [f\"{col}_vec\" for col in categorical_cols] + [\"temp_buckets\"]\n",
        "    vector_assembler = VectorAssembler(\n",
        "        inputCols=assembler_inputs,\n",
        "        outputCol=\"features\",\n",
        "        handleInvalid=\"skip\"  # Skip invalid records\n",
        "    )\n",
        "    stages.append(vector_assembler)\n",
        "\n",
        "    # Create and apply the pipeline\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "    pipeline_model = pipeline.fit(df)\n",
        "    transformed_df = pipeline_model.transform(df)\n",
        "\n",
        "    log_process(\"Transformers applied successfully\")\n",
        "\n",
        "    # Show resulting schema\n",
        "    log_process(\"Transformed data schema:\")\n",
        "    transformed_df.printSchema()\n",
        "\n",
        "    return transformed_df, pipeline_model\n",
        "\n",
        "# Write transformed data to Delta Silver ML-ready table\n",
        "def write_to_silver_ml_ready(df):\n",
        "    \"\"\"Write the transformed dataframe to a Delta Lake Silver ML-ready table.\"\"\"\n",
        "    log_process(\"Writing ML-ready data to Delta Lake Silver table\")\n",
        "\n",
        "    # Select only necessary columns for ML\n",
        "    ml_ready_cols = [\n",
        "        \"dt\", \"year\", \"month\", \"season\", \"City\", \"Country\",\n",
        "        \"AverageTemperature\", \"AverageTemperatureUncertainty\",\n",
        "        # Temporal features\n",
        "        \"month_sin\", \"month_cos\", \"quarter\", \"is_leap_year\",\n",
        "        # Location features\n",
        "        \"latitude_numeric\", \"longitude_numeric\", \"hemisphere\", \"climate_zone\", \"distance_from_equator\", \"latitude_bin\",\n",
        "        # Temperature features\n",
        "        \"temp_category\", \"temp_anomaly\", \"temp_zscore\", \"temp_trend\", \"temp_5yr_stddev\",\n",
        "        # Transformer outputs\n",
        "        \"features\"\n",
        "    ]\n",
        "\n",
        "    ml_ready_df = df.select(*ml_ready_cols)\n",
        "\n",
        "    # Create a directory for our Delta tables if it doesn't exist\n",
        "    silver_ml_path = \"delta_lake/silver/ml_ready_weather_data\"\n",
        "\n",
        "    try:\n",
        "        # Write to Delta table partitioned by year\n",
        "        ml_ready_df.write \\\n",
        "            .format(\"delta\") \\\n",
        "            .partitionBy(\"year\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .save(silver_ml_path)\n",
        "\n",
        "        log_process(f\"ML-ready data written to Delta Lake Silver table at {silver_ml_path}\")\n",
        "        format_used = \"delta\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to Delta table: {str(e)}\")\n",
        "        print(\"Falling back to Parquet format...\")\n",
        "\n",
        "        # Fallback to Parquet if Delta fails\n",
        "        silver_ml_path = \"parquet_data/silver/ml_ready_weather_data\"\n",
        "        ml_ready_df.write \\\n",
        "            .format(\"parquet\") \\\n",
        "            .partitionBy(\"year\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .save(silver_ml_path)\n",
        "\n",
        "        log_process(f\"ML-ready data written to Parquet files at {silver_ml_path}\")\n",
        "        format_used = \"parquet\"\n",
        "\n",
        "    return silver_ml_path, format_used\n",
        "\n",
        "# Main process\n",
        "def run_feature_engineering_process():\n",
        "    \"\"\"Run the full Feature Engineering process.\"\"\"\n",
        "    log_process(\"Starting Feature Engineering process\")\n",
        "\n",
        "    # Initialize Spark\n",
        "    spark = create_spark_session()\n",
        "\n",
        "    # Load data from Silver table\n",
        "    silver_df = load_silver_data(spark)\n",
        "\n",
        "    # Register a temp view for SQL operations\n",
        "    silver_df.createOrReplaceTempView(\"weather_data\")\n",
        "\n",
        "    # Create features\n",
        "    df_with_temporal = create_temporal_features(silver_df)\n",
        "    df_with_location = create_location_features(df_with_temporal)\n",
        "    df_with_temp = create_temperature_features(df_with_location)\n",
        "\n",
        "    # Apply transformers\n",
        "    transformed_df, pipeline_model = apply_transformers(df_with_temp)\n",
        "\n",
        "    # Show sample of transformed data\n",
        "    log_process(\"Sample of transformed data:\")\n",
        "    transformed_df.select(\"dt\", \"City\", \"Country\", \"AverageTemperature\", \"features\").show(5, truncate=True)\n",
        "\n",
        "    # Write to ML-ready Silver table\n",
        "    silver_ml_path, format_used = write_to_silver_ml_ready(transformed_df)\n",
        "\n",
        "    # Verify ML-ready table\n",
        "    log_process(f\"Verifying {format_used} ML-ready table at {silver_ml_path}\")\n",
        "    if format_used == \"delta\":\n",
        "        verified_df = spark.read.format(\"delta\").load(silver_ml_path)\n",
        "    else:\n",
        "        verified_df = spark.read.format(\"parquet\").load(silver_ml_path)\n",
        "\n",
        "    log_process(f\"ML-ready table record count: {verified_df.count()}\")\n",
        "    log_process(\"Sample of ML-ready table:\")\n",
        "    verified_df.select(\"dt\", \"City\", \"Country\", \"AverageTemperature\", \"features\").show(5, truncate=True)\n",
        "\n",
        "    # Save the pipeline model for future use\n",
        "    pipeline_model_path = \"models/feature_engineering_pipeline\"\n",
        "    try:\n",
        "        pipeline_model.save(pipeline_model_path)\n",
        "        log_process(f\"Pipeline model saved to {pipeline_model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving pipeline model: {str(e)}\")\n",
        "\n",
        "    log_process(\"Feature Engineering process completed successfully\")\n",
        "\n",
        "    return spark, verified_df, pipeline_model\n",
        "\n",
        "# Run the process if this notebook is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        spark, ml_ready_df, pipeline_model = run_feature_engineering_process()\n",
        "        print(\"\\nFeature Engineering process executed successfully!\")\n",
        "        print(\"You can now use 'spark', 'ml_ready_df', and 'pipeline_model' variables in subsequent cells.\")\n",
        "\n",
        "        # Display detailed feature information\n",
        "        features_count = len(ml_ready_df.select(\"features\").first()[0])\n",
        "        print(f\"\\nCreated {features_count} ML-ready features\")\n",
        "        print(\"\\nFeature engineering steps completed:\")\n",
        "        print(\"1. Created temporal features (month_sin, month_cos, etc.)\")\n",
        "        print(\"2. Created location-based features (climate_zone, distance_from_equator, etc.)\")\n",
        "        print(\"3. Created temperature-derived features (temp_zscore, temp_trend, etc.)\")\n",
        "        print(\"4. Applied transformers (StringIndexer, OneHotEncoder, VectorAssembler)\")\n",
        "        print(\"5. Generated final feature vector\")\n",
        "        print(\"\\nData is now ready for machine learning model development\")\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"\\nERROR: Feature Engineering process failed with error: {str(e)}\")\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p46_wBR9t7He",
        "outputId": "af6bc341-70ca-4a17-f0ce-e9b4d5f20243"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-06 14:07:37] Starting Feature Engineering process\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(http://595cb6717a02:4040, \"/\", \"https://localhost:http://595cb6717a02:4040/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Web UI available at: http://595cb6717a02:4040\n",
            "[2025-05-06 14:07:37] Loading data from Delta Lake Silver table\n",
            "[2025-05-06 14:07:39] Loaded 467690 records from Delta Lake\n",
            "[2025-05-06 14:07:39] Creating temporal features\n",
            "[2025-05-06 14:07:39] Temporal features created\n",
            "[2025-05-06 14:07:39] Creating location-based features\n",
            "[2025-05-06 14:07:39] Location features created\n",
            "[2025-05-06 14:07:39] Creating temperature-derived features\n",
            "[2025-05-06 14:07:39] Temperature features created\n",
            "[2025-05-06 14:07:39] Applying PySpark transformers\n",
            "[2025-05-06 14:07:56] Transformers applied successfully\n",
            "[2025-05-06 14:07:56] Transformed data schema:\n",
            "root\n",
            " |-- dt: date (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- season: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- hemisphere: string (nullable = true)\n",
            " |-- latitude_numeric: float (nullable = true)\n",
            " |-- longitude_numeric: float (nullable = true)\n",
            " |-- AverageTemperature: double (nullable = true)\n",
            " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
            " |-- yearly_global_avg_temp: double (nullable = true)\n",
            " |-- temp_anomaly: double (nullable = true)\n",
            " |-- day_of_month: integer (nullable = true)\n",
            " |-- day_of_week: integer (nullable = true)\n",
            " |-- day_of_year: integer (nullable = true)\n",
            " |-- week_of_year: integer (nullable = true)\n",
            " |-- quarter: integer (nullable = true)\n",
            " |-- month_sin: double (nullable = true)\n",
            " |-- month_cos: double (nullable = true)\n",
            " |-- decade: integer (nullable = true)\n",
            " |-- century: integer (nullable = true)\n",
            " |-- is_leap_year: integer (nullable = true)\n",
            " |-- climate_zone: string (nullable = false)\n",
            " |-- distance_from_equator: float (nullable = true)\n",
            " |-- longitude_abs: float (nullable = true)\n",
            " |-- latitude_bin: string (nullable = false)\n",
            " |-- yearly_city_max_temp: double (nullable = true)\n",
            " |-- yearly_city_min_temp: double (nullable = true)\n",
            " |-- yearly_city_temp_range: double (nullable = true)\n",
            " |-- temp_category: string (nullable = false)\n",
            " |-- global_temp_mean: double (nullable = true)\n",
            " |-- global_temp_stddev: double (nullable = true)\n",
            " |-- temp_zscore: double (nullable = true)\n",
            " |-- season_avg_temp: double (nullable = true)\n",
            " |-- temp_diff_from_season_avg: double (nullable = true)\n",
            " |-- temp_5yr_avg: double (nullable = true)\n",
            " |-- temp_trend: double (nullable = true)\n",
            " |-- temp_5yr_stddev: double (nullable = true)\n",
            " |-- season_idx: double (nullable = false)\n",
            " |-- hemisphere_idx: double (nullable = false)\n",
            " |-- climate_zone_idx: double (nullable = false)\n",
            " |-- temp_category_idx: double (nullable = false)\n",
            " |-- latitude_bin_idx: double (nullable = false)\n",
            " |-- season_vec: vector (nullable = true)\n",
            " |-- hemisphere_vec: vector (nullable = true)\n",
            " |-- climate_zone_vec: vector (nullable = true)\n",
            " |-- temp_category_vec: vector (nullable = true)\n",
            " |-- latitude_bin_vec: vector (nullable = true)\n",
            " |-- lat_long_interaction: float (nullable = true)\n",
            " |-- latitude_squared: double (nullable = true)\n",
            " |-- longitude_squared: double (nullable = true)\n",
            " |-- temp_lat_interaction: double (nullable = true)\n",
            " |-- temp_buckets: double (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n",
            "[2025-05-06 14:07:56] Sample of transformed data:\n",
            "+----------+--------+-------+------------------+--------------------+\n",
            "|        dt|    City|Country|AverageTemperature|            features|\n",
            "+----------+--------+-------+------------------+--------------------+\n",
            "|1743-11-01|A CORUÑA|  SPAIN|            10.779|[42.5900001525878...|\n",
            "|1743-12-01|A CORUÑA|  SPAIN|17.928990675585677|[42.5900001525878...|\n",
            "|1744-10-01|A CORUÑA|  SPAIN|            12.904|[42.5900001525878...|\n",
            "|1744-11-01|A CORUÑA|  SPAIN|            11.028|[42.5900001525878...|\n",
            "|1744-09-01|A CORUÑA|  SPAIN|            16.068|[42.5900001525878...|\n",
            "+----------+--------+-------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "[2025-05-06 14:08:03] Writing ML-ready data to Delta Lake Silver table\n",
            "[2025-05-06 14:08:36] ML-ready data written to Delta Lake Silver table at delta_lake/silver/ml_ready_weather_data\n",
            "[2025-05-06 14:08:36] Verifying delta ML-ready table at delta_lake/silver/ml_ready_weather_data\n",
            "[2025-05-06 14:08:38] ML-ready table record count: 467690\n",
            "[2025-05-06 14:08:38] Sample of ML-ready table:\n",
            "+----------+--------+-------+------------------+--------------------+\n",
            "|        dt|    City|Country|AverageTemperature|            features|\n",
            "+----------+--------+-------+------------------+--------------------+\n",
            "|1767-10-01|A CORUÑA|  SPAIN|            14.245|[0.0,0.0,0.0,0.0,...|\n",
            "|1767-11-01|A CORUÑA|  SPAIN|            13.015|[0.0,0.0,0.0,0.0,...|\n",
            "|1767-09-01|A CORUÑA|  SPAIN|             17.27|[0.0,0.0,0.0,0.0,...|\n",
            "|1767-05-01|A CORUÑA|  SPAIN|            13.352|[0.0,0.0,0.0,0.0,...|\n",
            "|1767-03-01|A CORUÑA|  SPAIN|            11.454|[0.0,0.0,0.0,0.0,...|\n",
            "+----------+--------+-------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "[2025-05-06 14:08:43] Pipeline model saved to models/feature_engineering_pipeline\n",
            "[2025-05-06 14:08:43] Feature Engineering process completed successfully\n",
            "\n",
            "Feature Engineering process executed successfully!\n",
            "You can now use 'spark', 'ml_ready_df', and 'pipeline_model' variables in subsequent cells.\n",
            "\n",
            "Created 28 ML-ready features\n",
            "\n",
            "Feature engineering steps completed:\n",
            "1. Created temporal features (month_sin, month_cos, etc.)\n",
            "2. Created location-based features (climate_zone, distance_from_equator, etc.)\n",
            "3. Created temperature-derived features (temp_zscore, temp_trend, etc.)\n",
            "4. Applied transformers (StringIndexer, OneHotEncoder, VectorAssembler)\n",
            "5. Generated final feature vector\n",
            "\n",
            "Data is now ready for machine learning model development\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Module 4: ML Model Training & Deployment\n",
        "This notebook builds a machine learning model to predict average temperatures for cities\n",
        "using the feature-engineered data from the previous modules.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.pipeline import Pipeline, PipelineModel\n",
        "from pyspark.sql.window import Window\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a Spark session with Delta Lake support\n",
        "def create_spark_session():\n",
        "    \"\"\"Create and return a Spark session with Delta Lake support.\"\"\"\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Weather Data ML Model\") \\\n",
        "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
        "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # For Google Colab, display Spark UI link\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        display(output.serve_kernel_port_as_window(spark.sparkContext.uiWebUrl))\n",
        "        print(f\"Spark Web UI available at: {spark.sparkContext.uiWebUrl}\")\n",
        "    except:\n",
        "        print(\"Spark UI link not available\")\n",
        "\n",
        "    return spark\n",
        "\n",
        "# Create a simple logging function\n",
        "def log_process(message):\n",
        "    \"\"\"Log processing information with timestamp.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "# Load ML-ready data\n",
        "def load_ml_ready_data(spark, ml_ready_path=\"delta_lake/silver/ml_ready_weather_data\", fallback_path=\"parquet_data/silver/ml_ready_weather_data\"):\n",
        "    \"\"\"Load ML-ready data from the Silver table.\"\"\"\n",
        "    try:\n",
        "        # Try loading from Delta first\n",
        "        log_process(\"Loading ML-ready data from Delta Lake Silver table\")\n",
        "        df = spark.read.format(\"delta\").load(ml_ready_path)\n",
        "        format_used = \"Delta Lake\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Delta table: {str(e)}\")\n",
        "        print(\"Falling back to Parquet format...\")\n",
        "\n",
        "        # Fallback to Parquet\n",
        "        df = spark.read.format(\"parquet\").load(fallback_path)\n",
        "        format_used = \"Parquet\"\n",
        "\n",
        "    log_process(f\"Loaded {df.count()} records from {format_used}\")\n",
        "    return df\n",
        "\n",
        "# Function to load the feature engineering pipeline model\n",
        "def load_pipeline_model(model_path=\"models/feature_engineering_pipeline\"):\n",
        "    \"\"\"Load the feature engineering pipeline model if it exists.\"\"\"\n",
        "    try:\n",
        "        log_process(f\"Loading pipeline model from {model_path}\")\n",
        "        pipeline_model = PipelineModel.load(model_path)\n",
        "        log_process(\"Pipeline model loaded successfully\")\n",
        "        return pipeline_model\n",
        "    except Exception as e:\n",
        "        log_process(f\"Could not load pipeline model: {str(e)}\")\n",
        "        log_process(\"Will need to create new feature transformations for new data\")\n",
        "        return None\n",
        "\n",
        "# Split data into training and testing sets\n",
        "def split_data(df, seed=42):\n",
        "    \"\"\"Split data into training and testing sets.\"\"\"\n",
        "    log_process(\"Splitting data into training and testing sets\")\n",
        "\n",
        "    # Use a stratified sampling approach based on hemisphere and season\n",
        "    train_df, test_df = df.randomSplit([0.8, 0.2], seed=seed)\n",
        "\n",
        "    log_process(f\"Training set size: {train_df.count()}\")\n",
        "    log_process(f\"Testing set size: {test_df.count()}\")\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Create a temperature classification model\n",
        "def train_temp_category_classifier(train_df, test_df):\n",
        "    \"\"\"Train a classifier to predict temperature category.\"\"\"\n",
        "    log_process(\"Training temperature category classifier\")\n",
        "\n",
        "    # Initialize Random Forest Classifier\n",
        "    rf = RandomForestClassifier(\n",
        "        labelCol=\"temp_category_idx\",\n",
        "        featuresCol=\"features\",\n",
        "        numTrees=100,\n",
        "        maxDepth=10,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    log_process(\"Training Random Forest temperature category classifier\")\n",
        "    rf_model = rf.fit(train_df)\n",
        "\n",
        "    # Make predictions\n",
        "    log_process(\"Making predictions on test data\")\n",
        "    predictions = rf_model.transform(test_df)\n",
        "\n",
        "    # Evaluate model\n",
        "    evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"temp_category_idx\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"accuracy\"\n",
        "    )\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "    log_process(f\"Temperature category classifier accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    log_process(\"Top 10 features by importance:\")\n",
        "    feature_importance = [(i, importance) for i, importance in enumerate(rf_model.featureImportances)]\n",
        "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "    for i, importance in feature_importance[:10]:\n",
        "        log_process(f\"Feature {i}: {importance:.4f}\")\n",
        "\n",
        "    return rf_model\n",
        "\n",
        "# Create a temperature regression model\n",
        "def train_temperature_regressor(train_df, test_df):\n",
        "    \"\"\"Train a regressor to predict average temperature.\"\"\"\n",
        "    log_process(\"Training temperature regression model\")\n",
        "\n",
        "    # Create parameter grid for hyperparameter tuning\n",
        "    lr = LinearRegression(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"AverageTemperature\",\n",
        "        maxIter=10,\n",
        "        regParam=0.3,\n",
        "        elasticNetParam=0.8\n",
        "    )\n",
        "\n",
        "    rf = RandomForestRegressor(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"AverageTemperature\",\n",
        "        numTrees=100,\n",
        "        maxDepth=10\n",
        "    )\n",
        "\n",
        "    gbt = GBTRegressor(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"AverageTemperature\",\n",
        "        maxIter=10,\n",
        "        maxDepth=5\n",
        "    )\n",
        "\n",
        "    # Dictionary to store models and their metrics\n",
        "    models = {\n",
        "        \"Linear Regression\": lr,\n",
        "        \"Random Forest\": rf,\n",
        "        \"Gradient Boosted Trees\": gbt\n",
        "    }\n",
        "\n",
        "    best_model = None\n",
        "    best_rmse = float('inf')\n",
        "    best_model_name = None\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for name, model in models.items():\n",
        "        log_process(f\"Training {name} model\")\n",
        "        trained_model = model.fit(train_df)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = trained_model.transform(test_df)\n",
        "\n",
        "        # Evaluate model with RMSE\n",
        "        evaluator = RegressionEvaluator(\n",
        "            labelCol=\"AverageTemperature\",\n",
        "            predictionCol=\"prediction\",\n",
        "            metricName=\"rmse\"\n",
        "        )\n",
        "        rmse = evaluator.evaluate(predictions)\n",
        "        log_process(f\"{name} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        # Evaluate model with R2\n",
        "        r2_evaluator = RegressionEvaluator(\n",
        "            labelCol=\"AverageTemperature\",\n",
        "            predictionCol=\"prediction\",\n",
        "            metricName=\"r2\"\n",
        "        )\n",
        "        r2 = r2_evaluator.evaluate(predictions)\n",
        "        log_process(f\"{name} R²: {r2:.4f}\")\n",
        "\n",
        "        # Keep track of the best model\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            best_model = trained_model\n",
        "            best_model_name = name\n",
        "\n",
        "    log_process(f\"Best model: {best_model_name} with RMSE: {best_rmse:.4f}\")\n",
        "\n",
        "    # Plot actual vs predicted temperatures for the best model\n",
        "    log_process(\"Creating actual vs predicted plot for best model\")\n",
        "    predictions = best_model.transform(test_df)\n",
        "\n",
        "    # Sample for visualization\n",
        "    pred_sample = predictions.select(\n",
        "        \"City\", \"Country\", \"dt\", \"AverageTemperature\", \"prediction\"\n",
        "    ).sample(False, 0.1, seed=42).toPandas()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(pred_sample[\"AverageTemperature\"], pred_sample[\"prediction\"], alpha=0.5)\n",
        "    plt.plot([pred_sample[\"AverageTemperature\"].min(), pred_sample[\"AverageTemperature\"].max()],\n",
        "             [pred_sample[\"AverageTemperature\"].min(), pred_sample[\"AverageTemperature\"].max()],\n",
        "             'r--')\n",
        "    plt.xlabel(\"Actual Temperature\")\n",
        "    plt.ylabel(\"Predicted Temperature\")\n",
        "    plt.title(f\"Actual vs Predicted Temperature ({best_model_name})\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    return best_model, best_model_name, best_rmse\n",
        "\n",
        "# Function to save models\n",
        "def save_models(temp_category_model, temp_regressor_model):\n",
        "    \"\"\"Save the trained models to disk.\"\"\"\n",
        "    log_process(\"Saving models to disk\")\n",
        "\n",
        "    # Create models directory if it doesn't exist\n",
        "    models_dir = \"models\"\n",
        "    if not os.path.exists(models_dir):\n",
        "        os.makedirs(models_dir)\n",
        "\n",
        "    # Save classification model\n",
        "    classifier_path = os.path.join(models_dir, \"temp_category_classifier\")\n",
        "    temp_category_model.write().overwrite().save(classifier_path)\n",
        "    log_process(f\"Temperature category classifier saved to {classifier_path}\")\n",
        "\n",
        "    # Save regression model\n",
        "    regressor_path = os.path.join(models_dir, \"temp_regressor\")\n",
        "    temp_regressor_model.write().overwrite().save(regressor_path)\n",
        "    log_process(f\"Temperature regressor saved to {regressor_path}\")\n",
        "\n",
        "    return classifier_path, regressor_path\n",
        "\n",
        "# Function to load saved models\n",
        "def load_models():\n",
        "    \"\"\"Load the trained models from disk.\"\"\"\n",
        "    log_process(\"Loading models from disk\")\n",
        "\n",
        "    models_dir = \"models\"\n",
        "    classifier_path = os.path.join(models_dir, \"temp_category_classifier\")\n",
        "    regressor_path = os.path.join(models_dir, \"temp_regressor\")\n",
        "\n",
        "    try:\n",
        "        # Load classification model\n",
        "        from pyspark.ml.classification import RandomForestClassificationModel\n",
        "        temp_category_model = RandomForestClassificationModel.load(classifier_path)\n",
        "        log_process(f\"Temperature category classifier loaded from {classifier_path}\")\n",
        "\n",
        "        # Load regression model\n",
        "        from pyspark.ml.regression import RandomForestRegressionModel, LinearRegressionModel, GBTRegressionModel\n",
        "\n",
        "        # Try loading different types of models (we don't know which one was the best)\n",
        "        try:\n",
        "            temp_regressor_model = RandomForestRegressionModel.load(regressor_path)\n",
        "            model_type = \"Random Forest\"\n",
        "        except:\n",
        "            try:\n",
        "                temp_regressor_model = GBTRegressionModel.load(regressor_path)\n",
        "                model_type = \"Gradient Boosted Trees\"\n",
        "            except:\n",
        "                temp_regressor_model = LinearRegressionModel.load(regressor_path)\n",
        "                model_type = \"Linear Regression\"\n",
        "\n",
        "        log_process(f\"Temperature regressor ({model_type}) loaded from {regressor_path}\")\n",
        "\n",
        "        return temp_category_model, temp_regressor_model, model_type\n",
        "    except Exception as e:\n",
        "        log_process(f\"Error loading models: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Function to prepare city input data\n",
        "def prepare_city_data(spark, city, country, month=None, year=None):\n",
        "    \"\"\"Prepare input data for a given city for prediction.\"\"\"\n",
        "    log_process(f\"Preparing data for {city}, {country}\")\n",
        "\n",
        "    # Load the ML-ready data\n",
        "    ml_ready_df = load_ml_ready_data(spark)\n",
        "\n",
        "    # Filter for the specified city\n",
        "    city_data = ml_ready_df.filter(\n",
        "        (F.col(\"City\") == city.upper()) &\n",
        "        (F.col(\"Country\") == country.upper())\n",
        "    )\n",
        "\n",
        "    # If no data found for this city\n",
        "    if city_data.count() == 0:\n",
        "        log_process(f\"No data found for {city}, {country}\")\n",
        "        return None\n",
        "\n",
        "    # Filter for specific month and year if provided\n",
        "    if month is not None:\n",
        "        city_data = city_data.filter(F.col(\"month\") == month)\n",
        "\n",
        "    if year is not None:\n",
        "        city_data = city_data.filter(F.col(\"year\") == year)\n",
        "\n",
        "    # Get the most recent record for this city if no specific month/year\n",
        "    if month is None and year is None:\n",
        "        city_data = city_data.orderBy(F.col(\"dt\").desc()).limit(1)\n",
        "\n",
        "    # Return the filtered data\n",
        "    return city_data\n",
        "\n",
        "# Function to make predictions for a city\n",
        "def predict_city_temperature(spark, city, country, month=None, year=None, temp_category_model=None, temp_regressor_model=None):\n",
        "    \"\"\"Make temperature predictions for a specified city.\"\"\"\n",
        "    log_process(f\"Making predictions for {city}, {country}\")\n",
        "\n",
        "    # If models weren't provided, try to load them\n",
        "    if temp_category_model is None or temp_regressor_model is None:\n",
        "        temp_category_model, temp_regressor_model, model_type = load_models()\n",
        "\n",
        "        if temp_category_model is None or temp_regressor_model is None:\n",
        "            log_process(\"Failed to load models. Please train models first.\")\n",
        "            return None\n",
        "\n",
        "    # Prepare city data for prediction\n",
        "    city_data = prepare_city_data(spark, city, country, month, year)\n",
        "\n",
        "    if city_data is None or city_data.count() == 0:\n",
        "        log_process(f\"No historical data available for {city}, {country}\")\n",
        "        return None\n",
        "\n",
        "    # Make predictions\n",
        "    category_predictions = temp_category_model.transform(city_data)\n",
        "    temp_predictions = temp_regressor_model.transform(category_predictions)\n",
        "\n",
        "    # Extract and return prediction results\n",
        "    results = temp_predictions.select(\n",
        "        \"dt\", \"year\", \"month\", \"season\", \"City\", \"Country\",\n",
        "        \"AverageTemperature\", \"temp_category\", \"prediction\"\n",
        "    ).collect()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to list available cities\n",
        "def list_available_cities(spark, limit=20):\n",
        "    \"\"\"List available cities in the dataset.\"\"\"\n",
        "    log_process(\"Listing available cities\")\n",
        "\n",
        "    ml_ready_df = load_ml_ready_data(spark)\n",
        "\n",
        "    # Get distinct cities with their countries\n",
        "    cities_df = ml_ready_df.select(\"City\", \"Country\").distinct()\n",
        "\n",
        "    # Display available cities\n",
        "    log_process(f\"Found {cities_df.count()} unique cities\")\n",
        "    log_process(\"Sample of available cities:\")\n",
        "    cities_df.show(limit)\n",
        "\n",
        "    return cities_df\n",
        "\n",
        "# Interactive prediction function\n",
        "def interactive_prediction(spark, temp_category_model=None, temp_regressor_model=None):\n",
        "    \"\"\"Interactive function to get user input and make predictions.\"\"\"\n",
        "    log_process(\"Starting interactive prediction\")\n",
        "\n",
        "    # If models weren't provided, try to load them\n",
        "    if temp_category_model is None or temp_regressor_model is None:\n",
        "        temp_category_model, temp_regressor_model, model_type = load_models()\n",
        "\n",
        "        if temp_category_model is None or temp_regressor_model is None:\n",
        "            log_process(\"Failed to load models. Please train models first.\")\n",
        "            return None\n",
        "\n",
        "    # List some available cities\n",
        "    cities_df = list_available_cities(spark)\n",
        "\n",
        "    # Get user input\n",
        "    print(\"\\n=== Temperature Prediction System ===\")\n",
        "    print(\"Enter city and country information:\")\n",
        "\n",
        "    city = input(\"City name: \")\n",
        "    country = input(\"Country name: \")\n",
        "\n",
        "    # Optional filters\n",
        "    print(\"\\nOptional filters (leave blank to use most recent data):\")\n",
        "    month_input = input(\"Month (1-12, leave blank for most recent): \")\n",
        "    year_input = input(\"Year (e.g. 2023, leave blank for most recent): \")\n",
        "\n",
        "    # Process inputs\n",
        "    month = int(month_input) if month_input.strip() else None\n",
        "    year = int(year_input) if year_input.strip() else None\n",
        "\n",
        "    # Make prediction\n",
        "    results = predict_city_temperature(spark, city, country, month, year,\n",
        "                                      temp_category_model, temp_regressor_model)\n",
        "\n",
        "    if results is None or len(results) == 0:\n",
        "        print(f\"\\nNo data or predictions available for {city}, {country}\")\n",
        "        return None\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n=== Prediction Results ===\")\n",
        "    for row in results:\n",
        "        print(f\"City: {row['City']}, {row['Country']}\")\n",
        "        print(f\"Date: {row['dt']} (Year: {row['year']}, Month: {row['month']}, Season: {row['season']})\")\n",
        "        print(f\"Historical Average Temperature: {row['AverageTemperature']:.2f}°C\")\n",
        "        print(f\"Temperature Category: {row['temp_category']}\")\n",
        "        print(f\"Predicted Temperature: {row['prediction']:.2f}°C\")\n",
        "        print(f\"Prediction Error: {abs(row['AverageTemperature'] - row['prediction']):.2f}°C\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to create a historical temperature chart for a city\n",
        "def city_temperature_history(spark, city, country):\n",
        "    \"\"\"Create a historical temperature chart for a specified city.\"\"\"\n",
        "    log_process(f\"Creating temperature history chart for {city}, {country}\")\n",
        "\n",
        "    # Load the ML-ready data\n",
        "    ml_ready_df = load_ml_ready_data(spark)\n",
        "\n",
        "    # Filter for the specified city\n",
        "    city_data = ml_ready_df.filter(\n",
        "        (F.col(\"City\") == city.upper()) &\n",
        "        (F.col(\"Country\") == country.upper())\n",
        "    )\n",
        "\n",
        "    # If no data found for this city\n",
        "    if city_data.count() == 0:\n",
        "        log_process(f\"No data found for {city}, {country}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate yearly average temperatures\n",
        "    yearly_temps = city_data.groupBy(\"year\").agg(\n",
        "        F.avg(\"AverageTemperature\").alias(\"avg_temp\"),\n",
        "        F.min(\"AverageTemperature\").alias(\"min_temp\"),\n",
        "        F.max(\"AverageTemperature\").alias(\"max_temp\")\n",
        "    ).orderBy(\"year\")\n",
        "\n",
        "    # Convert to Pandas for plotting\n",
        "    yearly_temps_pd = yearly_temps.toPandas()\n",
        "\n",
        "    if len(yearly_temps_pd) == 0:\n",
        "        log_process(\"No yearly data available for plotting\")\n",
        "        return None\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(yearly_temps_pd[\"year\"], yearly_temps_pd[\"avg_temp\"], 'b-', label=\"Yearly Average\")\n",
        "    plt.fill_between(yearly_temps_pd[\"year\"],\n",
        "                   yearly_temps_pd[\"min_temp\"],\n",
        "                   yearly_temps_pd[\"max_temp\"],\n",
        "                   alpha=0.2, color='blue')\n",
        "\n",
        "    # Add trend line\n",
        "    if len(yearly_temps_pd) > 1:\n",
        "        from scipy import stats\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
        "            yearly_temps_pd[\"year\"], yearly_temps_pd[\"avg_temp\"]\n",
        "        )\n",
        "        trend_line = slope * yearly_temps_pd[\"year\"] + intercept\n",
        "        plt.plot(yearly_temps_pd[\"year\"], trend_line, 'r--',\n",
        "                label=f\"Trend (Slope: {slope:.4f}°C/year)\")\n",
        "\n",
        "    plt.title(f\"Historical Temperature for {city}, {country}\")\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Temperature (°C)\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Return trend statistics\n",
        "    trend_stats = {\n",
        "        \"city\": city,\n",
        "        \"country\": country,\n",
        "        \"data_points\": len(yearly_temps_pd),\n",
        "        \"year_range\": (yearly_temps_pd[\"year\"].min(), yearly_temps_pd[\"year\"].max()),\n",
        "        \"slope\": slope if len(yearly_temps_pd) > 1 else None,\n",
        "        \"r_value\": r_value if len(yearly_temps_pd) > 1 else None,\n",
        "        \"p_value\": p_value if len(yearly_temps_pd) > 1 else None\n",
        "    }\n",
        "\n",
        "    return trend_stats\n",
        "\n",
        "# Function to deploy a prediction web app\n",
        "def create_prediction_app():\n",
        "    \"\"\"Create a simple web app for temperature prediction.\"\"\"\n",
        "    log_process(\"This function would create a web app for predictions\")\n",
        "    log_process(\"However, web deployment needs external frameworks not available in this environment\")\n",
        "    log_process(\"Consider using Flask or Streamlit with the trained models for deployment\")\n",
        "\n",
        "    # Pseudocode for Flask app\n",
        "    pseudo_code = \"\"\"\n",
        "    # Flask App Pseudocode\n",
        "    from flask import Flask, request, render_template\n",
        "    app = Flask(__name__)\n",
        "\n",
        "    @app.route('/')\n",
        "    def home():\n",
        "        return render_template('index.html')\n",
        "\n",
        "    @app.route('/predict', methods=['POST'])\n",
        "    def predict():\n",
        "        city = request.form.get('city')\n",
        "        country = request.form.get('country')\n",
        "        month = request.form.get('month')\n",
        "        year = request.form.get('year')\n",
        "\n",
        "        # Call prediction function\n",
        "        results = predict_city_temperature(spark, city, country, month, year)\n",
        "\n",
        "        return render_template('results.html', results=results)\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        # Initialize Spark and load models\n",
        "        spark = create_spark_session()\n",
        "        temp_category_model, temp_regressor_model, model_type = load_models()\n",
        "        app.run(debug=True)\n",
        "    \"\"\"\n",
        "\n",
        "    print(pseudo_code)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Main process\n",
        "def run_ml_model_training():\n",
        "    \"\"\"Run the ML model training process.\"\"\"\n",
        "    log_process(\"Starting ML Model Training & Deployment process\")\n",
        "\n",
        "    # Initialize Spark\n",
        "    spark = create_spark_session()\n",
        "\n",
        "    # Load ML-ready data\n",
        "    ml_ready_df = load_ml_ready_data(spark)\n",
        "\n",
        "    # Split data\n",
        "    train_df, test_df = split_data(ml_ready_df)\n",
        "\n",
        "    # Train temperature category classifier\n",
        "    temp_category_model = train_temp_category_classifier(train_df, test_df)\n",
        "\n",
        "    # Train temperature regressor\n",
        "    temp_regressor_model, model_name, rmse = train_temperature_regressor(train_df, test_df)\n",
        "\n",
        "    # Save models\n",
        "    classifier_path, regressor_path = save_models(temp_category_model, temp_regressor_model)\n",
        "\n",
        "    log_process(\"ML Model Training & Deployment process completed successfully\")\n",
        "\n",
        "    return spark, temp_category_model, temp_regressor_model\n",
        "\n",
        "# Function to run interactive prediction system\n",
        "def run_prediction_system():\n",
        "    \"\"\"Run the interactive prediction system.\"\"\"\n",
        "    log_process(\"Starting Prediction System\")\n",
        "\n",
        "    # Initialize Spark\n",
        "    spark = create_spark_session()\n",
        "\n",
        "    # Load models\n",
        "    temp_category_model, temp_regressor_model, model_type = load_models()\n",
        "\n",
        "    if temp_category_model is None or temp_regressor_model is None:\n",
        "        log_process(\"No saved models found. Training new models...\")\n",
        "        spark, temp_category_model, temp_regressor_model = run_ml_model_training()\n",
        "\n",
        "    # Start interactive prediction\n",
        "    interactive_prediction(spark, temp_category_model, temp_regressor_model)\n",
        "\n",
        "    return spark, temp_category_model, temp_regressor_model\n",
        "\n",
        "# Run the process if this notebook is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        print(\"\\n=== City Temperature Prediction System ===\")\n",
        "        print(\"1. Train ML models\")\n",
        "        print(\"2. Make predictions\")\n",
        "        print(\"3. Show city temperature history\")\n",
        "        print(\"4. List available cities\")\n",
        "        print(\"5. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-5): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            spark, temp_category_model, temp_regressor_model = run_ml_model_training()\n",
        "            print(\"\\nML models trained and saved successfully!\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            spark, temp_category_model, temp_regressor_model = run_prediction_system()\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            spark = create_spark_session()\n",
        "            city = input(\"Enter city name: \")\n",
        "            country = input(\"Enter country name: \")\n",
        "            trend_stats = city_temperature_history(spark, city, country)\n",
        "            if trend_stats and trend_stats[\"slope\"]:\n",
        "                print(f\"\\nTemperature trend: {trend_stats['slope']:.4f}°C/year\")\n",
        "                print(f\"Statistical significance (p-value): {trend_stats['p_value']:.4f}\")\n",
        "\n",
        "        elif choice == \"4\":\n",
        "            spark = create_spark_session()\n",
        "            limit = int(input(\"How many cities to show? \"))\n",
        "            list_available_cities(spark, limit)\n",
        "\n",
        "        elif choice == \"5\":\n",
        "            print(\"Exiting...\")\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please select 1-5.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"\\nERROR: Process failed with error: {str(e)}\")\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "a88kMpCwNm4T",
        "outputId": "698babe9-8694-4581-9945-bcd46c1fd2c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== City Temperature Prediction System ===\n",
            "1. Train ML models\n",
            "2. Make predictions\n",
            "3. Show city temperature history\n",
            "4. List available cities\n",
            "5. Exit\n",
            "Enter your choice (1-5): 1\n",
            "[2025-05-06 15:30:52] Starting ML Model Training & Deployment process\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(http://595cb6717a02:4040, \"/\", \"https://localhost:http://595cb6717a02:4040/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Web UI available at: http://595cb6717a02:4040\n",
            "[2025-05-06 15:30:52] Loading ML-ready data from Delta Lake Silver table\n",
            "[2025-05-06 15:30:59] Loaded 467690 records from Delta Lake\n",
            "[2025-05-06 15:30:59] Splitting data into training and testing sets\n",
            "[2025-05-06 15:31:11] Training set size: 374198\n",
            "[2025-05-06 15:31:17] Testing set size: 93492\n",
            "[2025-05-06 15:31:17] Training temperature category classifier\n",
            "[2025-05-06 15:31:18] Training Random Forest temperature category classifier\n",
            "\n",
            "ERROR: Process failed with error: temp_category_idx does not exist. Available: dt, year, month, season, City, Country, AverageTemperature, AverageTemperatureUncertainty, month_sin, month_cos, quarter, is_leap_year, latitude_numeric, longitude_numeric, hemisphere, climate_zone, distance_from_equator, latitude_bin, temp_category, temp_anomaly, temp_zscore, temp_trend, temp_5yr_stddev, features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-9-ad0c5d69e022>\", line 595, in <cell line: 0>\n",
            "    spark, temp_category_model, temp_regressor_model = run_ml_model_training()\n",
            "                                                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ad0c5d69e022>\", line 550, in run_ml_model_training\n",
            "    temp_category_model = train_temp_category_classifier(train_df, test_df)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ad0c5d69e022>\", line 111, in train_temp_category_classifier\n",
            "    rf_model = rf.fit(train_df)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\", line 205, in fit\n",
            "    return self._fit(dataset)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
            "    java_model = self._fit_java(dataset)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
            "    return self._java_obj.fit(dataset._jdf)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "                   ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
            "    raise converted from None\n",
            "pyspark.errors.exceptions.captured.IllegalArgumentException: temp_category_idx does not exist. Available: dt, year, month, season, City, Country, AverageTemperature, AverageTemperatureUncertainty, month_sin, month_cos, quarter, is_leap_year, latitude_numeric, longitude_numeric, hemisphere, climate_zone, distance_from_equator, latitude_bin, temp_category, temp_anomaly, temp_zscore, temp_trend, temp_5yr_stddev, features\n"
          ]
        }
      ]
    }
  ]
}